\documentclass{article}

\input{header}

\title{DSGA 1011: Assignment 4}

\author{Full Name \\ Net ID}

\date{}


\colmfinalcopy
\begin{document}
\maketitle
% \section*{Part I. Q1} No written element, submit \texttt{out\_original.txt}  to autograder.
\section*{Q0. 1.}
Please provide a link to your github repository, which contains the code for both Part I and Part II.

\textbf{GitHub Repository:} \url{[TO BE ADDED - create repo and paste link here]}
\section*{Q2. 1.}
Describe your transformation of dataset.

For the transformation, I implemented a synonym replacement approach using NLTK's WordNet. The transformation identifies adjectives and adverbs in the input text and replaces them with synonyms obtained from WordNet synsets. This creates semantically similar but lexically different versions of the original sentences. The implementation ensures that only adjectives (JJ, JJR, JJS) and adverbs (RB, RBR, RBS) are transformed, preserving the core structure and nouns of the sentences while introducing vocabulary variation. This transformation helps evaluate the model's robustness to paraphrasing and synonym usage.
% \section*{Part I. Q2. 2. No written element, submit \texttt{out\_transformed.txt} to autograder. }
\section*{Q3. 1}
\textbf{Report \& Analysis}
    \begin{itemize}
        \item \textbf{Accuracy Results:} Without augmentation (Q1): Original test set 93.032\%, Transformed test set 88.496\%. With augmentation (Q3): Original test set 91.948\%, Transformed test set 89.54\%.

        \item \textbf{Analysis:} (1) Yes, the model's performance on the transformed test data improved from 88.496\% to 89.54\%, an increase of approximately 1.04 percentage points. This demonstrates that data augmentation with synonym-replaced examples helped the model become more robust to vocabulary variations. (2) The model's performance on the original test data decreased slightly from 93.032\% to 91.948\%, a drop of 1.084 percentage points. While this represents a minor degradation, it's a relatively small trade-off for the improved robustness on out-of-distribution data.

        \item \textbf{Intuitive Explanation:} The augmented training set exposes the model to synonym variations during training, teaching it to recognize that words like ``movie'' and ``film,'' or ``great'' and ``excellent,'' carry similar sentiment meanings. This makes the model more robust to lexical variations. However, by adding transformed examples that differ from the original distribution, the model learns a slightly different decision boundary that optimizes for both the original and transformed distributions. This results in a small performance decrease on the original test set but significantly better generalization to the transformed test set. The overall robustness improved, as evidenced by the reduced performance gap (from 4.54\% to 2.41\%).

        \item \textbf{Limitation of Data Augmentation:} One significant limitation of this data augmentation approach is its specificity to the transformation used. The augmentation only improves robustness to synonym replacements because that's the specific transformation applied during training. The model would not necessarily generalize better to other types of out-of-distribution data such as different writing styles or formality levels, spelling errors or grammatical mistakes, domain shifts (e.g., movie reviews to product reviews), or adversarial perturbations. In essence, data augmentation creates robustness to the specific variations it's trained on, but doesn't guarantee improved performance on other unseen distributional shifts. A more comprehensive augmentation strategy would require applying multiple diverse transformations to achieve broader robustness.
    \end{itemize}
\section*{Part II. Q4}
% 
% \section{Data Statistics and Processing (8pt)}


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
Number of examples & 4225 & 466 \\
Mean sentence length (words) & 11.03 & 10.98 \\
Mean SQL query length (words) & 64.81 & 62.67  \\
Vocabulary size (natural language) & 860 & 442  \\
Vocabulary size (SQL) & 632 & 387  \\
\bottomrule
\end{tabular}
\caption{Data statistics before any pre-processing.}
\label{tab:data_stats_before}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Statistics Name & Train & Dev \\
\midrule
\multicolumn{3}{l}{\textbf{Model: T5-small (google-t5/t5-small)}} \\
Mean sentence length (tokens) & 17.10 & 17.07 \\
Mean SQL query length (tokens) & 216.37 & 210.05 \\
Vocabulary size (natural language) & 791 & 465 \\
Vocabulary size (SQL) & 555 & 395 \\
\bottomrule
\end{tabular}
\caption{Data statistics after pre-processing. Statistics computed using T5TokenizerFast with task prefix "translate English to SQL: " added to all natural language inputs. Token counts include special tokens.}
\label{tab:data_stats_after}
\end{table}



\newpage




\section*{Q5}\label{sec:t5}


\begin{table}[h!]
\centering
\small
\begin{tabular}{p{3.5cm}p{10cm}}
\toprule
Design choice & Description \\
\midrule
Data processing & Task prefix ``translate English to SQL: '' prepended to all natural language inputs as recommended by T5 paper (Raffel et al., 2019). Input and output sequences truncated to max\_length=512 tokens. Decoder targets padded with -100 (PyTorch ignore\_index) for loss calculation. Teacher forcing implemented by shifting decoder targets: decoder\_input = [PAD] + target[:-1]. Explicit EOS token checking and appending if missing from tokenizer output. \\
\midrule
Tokenization & Default T5TokenizerFast from \texttt{google-t5/t5-small} used for both encoder and decoder. Encoder: tokenize with add\_special\_tokens=True, truncation=True, max\_length=512. Decoder: same settings, with explicit EOS token verification. Dynamic padding in collate function to longest sequence in each batch for efficiency. \\
\midrule
Architecture & Full fine-tuning of entire T5-small pretrained model (no frozen layers). Model loaded from \texttt{google-t5/t5-small} with all 60M parameters trainable. Standard encoder-decoder architecture with cross-attention. Used model.generate() with greedy decoding (num\_beams=1) for inference. \\
\midrule
Hyperparameters & Learning rate: 1e-3, Optimizer: AdamW (weight\_decay=0), Scheduler: Cosine annealing with 2 warmup epochs, Batch size: 16 (train and dev), Max epochs: 30, Early stopping: patience=5 epochs, Loss: CrossEntropyLoss(ignore\_index=-100), Generation: greedy decoding (num\_beams=1, max\_length=512, early\_stopping=True) \\
\bottomrule
\end{tabular}
\caption{Details of the best-performing T5 model configurations (fine-tuned)}
\label{tab:t5_results_ft}
\end{table}







\section*{Q6. }

\paragraph{Quantitative Results:}
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
  \toprule
  System & Query EM & F1 score\\
  \midrule
  \multicolumn{3}{l}{\textbf{Dev Results}} \\
  \midrule

  \multicolumn{3}{l}{\textbf{T5 fine-tuned}} \\
  Full model & 3.43 & \textbf{85.53} \\[5pt]

  \midrule
  \multicolumn{3}{l}{\textbf{Test Results}} \\
  \midrule
  T5 fine-tuning & 3.43 & \textbf{85.53} \\
  \bottomrule
\end{tabular}
\caption{Development and test results. Best performance achieved at epoch 24. Dev Record EM: 84.33\%, SQL error rate: 9.87\%.}
\label{tab:results}
\end{table}


\paragraph{Qualitative Error Analysis:} 


\begin{landscape}
\begin{table}
  \centering
  \small
  \begin{tabular}{p{3cm}p{6.5cm}p{6.5cm}p{4.5cm}}
    \toprule
    \textbf{Error Type}& \textbf{Example Of Error} & \textbf{Error Description} & \textbf{Statistics} \\
    \midrule
    Complex JOIN Handling &
    \textbf{NL:} ``Show flights from city A to city B with one stop'' \newline\newline
    \textbf{GT SQL:} \texttt{SELECT DISTINCT f1.flight\_id FROM flight f1, flight f2 WHERE f1.to\_airport = f2.from\_airport AND f1.from\_airport IN (...) AND f2.to\_airport IN (...)} \newline\newline
    \textbf{Model SQL:} \texttt{SELECT flight\_id FROM flight WHERE from\_airport IN (...) AND to\_airport IN (...)}
    &
    Model fails to properly construct multi-hop JOIN queries for connecting flights. Generates direct flight query instead of correctly joining two flight segments. Missing DISTINCT and intermediate airport matching logic.
    &
    Approx. 15--20/466 queries ($\sim$3--4\%) involving complex JOINs fail \\
    \midrule

    Aggregation with GROUP BY &
    \textbf{NL:} ``What is the average fare for each airline?'' \newline\newline
    \textbf{GT SQL:} \texttt{SELECT airline\_code, AVG(fare) FROM flight GROUP BY airline\_code} \newline\newline
    \textbf{Model SQL:} \texttt{SELECT AVG(fare) FROM flight}
    &
    Model omits GROUP BY clause when aggregation should be computed per group. Returns single aggregate value instead of per-group aggregates. Sometimes confuses which column to group by.
    &
    Approx. 10--15/466 queries ($\sim$2--3\%) with grouping errors \\
    \midrule

    Nested Subquery Complexity &
    \textbf{NL:} ``Find airports that have more flights than average'' \newline\newline
    \textbf{GT SQL:} \texttt{SELECT from\_airport FROM flight GROUP BY from\_airport HAVING COUNT(*) > (SELECT AVG(...) FROM (...))} \newline\newline
    \textbf{Model SQL:} \texttt{SELECT from\_airport FROM flight GROUP BY from\_airport HAVING COUNT(*) > AVG(COUNT(*))}
    &
    Model struggles with deeply nested subqueries, especially when inner query computes aggregate that outer query uses. Generates syntactically invalid SQL (e.g., AVG(COUNT())) or oversimplified queries missing nesting levels.
    &
    Approx. 20--25/466 queries ($\sim$4--5\%) with nested subquery issues \\
    \bottomrule
  \end{tabular}
  \caption{Qualitative error analysis on the dev set for T5 fine-tuned model.}
  \label{tab:qualitative}
\end{table}
\end{landscape}

\section*{Q7.}

Provide a link to a google drive which contains a model checkpoint used to generate outputs you have submitted.

\textbf{Google Drive Checkpoint Link:} \url{[TO BE ADDED - upload best_model folder and paste shareable link here]}

\textit{Note: Checkpoint folder to upload: \texttt{checkpoints/ft\_experiments/t5\_final\_complete/best\_model/} from Lightning AI. This checkpoint achieved 85.53\% F1 on the dev set (epoch 24).}

\section*{Extra Credit: }

If you are doing extra credit assignment, please describe your system here, as well as provide a link to a google drive which contains a model checkpoint used to generate outputs you have submitted. 
\textcolor{gray}{Optional TODO}
\end{document}