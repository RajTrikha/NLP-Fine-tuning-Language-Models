{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCCrJsiSc6PC",
        "outputId": "99d4035f-ed44-4cd4-9ff1-e776c03c19a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S3L7Dx72dHdg"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/hw4/part-1-code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "IZjzlyawdLpV",
        "outputId": "8de98bb9-8695-4576-b2a2-1b67808f6d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload main.py, utils.py, and requirements.txt one by one:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6549b4cd-37c8-4e29-96dd-ea04bda58292\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6549b4cd-37c8-4e29-96dd-ea04bda58292\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving main.py to main.py\n",
            "Saving README.md to README.md\n",
            "Saving requirements.txt to requirements.txt\n",
            "Saving utils.py to utils.py\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "os.chdir('/content/hw4/part-1-code/')\n",
        "print(\"Upload main.py, utils.py, and requirements.txt one by one:\")\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-qrBJtFdTfI",
        "outputId": "c4d251dd-ac26-4714-d6dc-a13e3ae1d054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-1-code/\n",
        "\n",
        "!pip install transformers datasets torch tqdm evaluate nltk scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2Rwg2Qrda7R",
        "outputId": "7846c3e4-5446-458f-f65c-798ae5554a8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE_RGyCMdgl8",
        "outputId": "79b64add-7c35-4f24-9d92-739e8e870417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "import random\n",
        "import argparse\n",
        "from utils import *\n",
        "import os\n",
        "\n",
        "# Set seed\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# Tokenize the input\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "# Core training function\n",
        "def do_train(args, model, train_dataloader, save_dir=\"./out\"):\n",
        "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
        "    num_epochs = args.num_epochs\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        "    )\n",
        "    model.train()\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "    ################################\n",
        "    ##### YOUR CODE BEGINGS HERE ###\n",
        "\n",
        "    # Implement the training loop --- make sure to use the optimizer and lr_sceduler (learning rate scheduler)\n",
        "    # Remember that pytorch uses gradient accumumlation so you need to use zero_grad (https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html)\n",
        "    # You can use progress_bar.update(1) to see the progress during training\n",
        "    # You can refer to the pytorch tutorial covered in class for reference\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_dataloader:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            # Zero gradients for next iteration\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE ######\n",
        "\n",
        "    print(\"Training completed...\")\n",
        "    print(\"Saving Model....\")\n",
        "    model.save_pretrained(save_dir)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# Core evaluation function\n",
        "def do_eval(eval_dataloader, output_dir, out_file):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    out_file = open(out_file, \"w\")\n",
        "\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "        # write to output file\n",
        "        for pred, label in zip(predictions, batch[\"labels\"]):\n",
        "                out_file.write(f\"{pred.item()}\\n\")\n",
        "                out_file.write(f\"{label.item()}\\n\")\n",
        "    out_file.close()\n",
        "    score = metric.compute()\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "# Created a dataladoer for the augmented training dataset\n",
        "def create_augmented_dataloader(args, dataset):\n",
        "    ################################\n",
        "    ##### YOUR CODE BEGINGS HERE ###\n",
        "\n",
        "    # Here, 'dataset' is the original dataset. You should return a dataloader called 'train_dataloader' -- this\n",
        "    # dataloader will be for the original training split augmented with 5k random transformed examples from the training set.\n",
        "    # You may find it helpful to see how the dataloader was created at other place in this code.\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE ######\n",
        "\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "# Create a dataloader for the transformed test set\n",
        "def create_transformed_dataloader(args, dataset, debug_transformation):\n",
        "    # Print 5 random transformed examples\n",
        "    if debug_transformation:\n",
        "        small_dataset = dataset[\"test\"].shuffle(seed=42).select(range(5))\n",
        "        small_transformed_dataset = small_dataset.map(custom_transform, load_from_cache_file=False)\n",
        "        for k in range(5):\n",
        "            print(\"Original Example \", str(k))\n",
        "            print(small_dataset[k])\n",
        "            print(\"\\n\")\n",
        "            print(\"Transformed Example \", str(k))\n",
        "            print(small_transformed_dataset[k])\n",
        "            print('=' * 30)\n",
        "\n",
        "        exit()\n",
        "\n",
        "    transformed_dataset = dataset[\"test\"].map(custom_transform, load_from_cache_file=False)\n",
        "    transformed_tokenized_dataset = transformed_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    transformed_tokenized_dataset = transformed_tokenized_dataset.remove_columns([\"text\"])\n",
        "    transformed_tokenized_dataset = transformed_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "    transformed_tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "    transformed_val_dataset = transformed_tokenized_dataset\n",
        "    eval_dataloader = DataLoader(transformed_val_dataset, batch_size=args.batch_size)\n",
        "\n",
        "    return eval_dataloader\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Arguments\n",
        "    parser.add_argument(\"--train\", action=\"store_true\", help=\"train a model on the training data\")\n",
        "    parser.add_argument(\"--train_augmented\", action=\"store_true\", help=\"train a model on the augmented training data\")\n",
        "    parser.add_argument(\"--eval\", action=\"store_true\", help=\"evaluate model on the test set\")\n",
        "    parser.add_argument(\"--eval_transformed\", action=\"store_true\", help=\"evaluate model on the transformed test set\")\n",
        "    parser.add_argument(\"--model_dir\", type=str, default=\"./out\")\n",
        "    parser.add_argument(\"--debug_train\", action=\"store_true\",\n",
        "                        help=\"use a subset for training to debug your training loop\")\n",
        "    parser.add_argument(\"--debug_transformation\", action=\"store_true\",\n",
        "                        help=\"print a few transformed examples for debugging\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
        "    parser.add_argument(\"--num_epochs\", type=int, default=3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    global device\n",
        "    global tokenizer\n",
        "\n",
        "    # Device\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Prepare dataset for use by model\n",
        "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "    tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "    small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(4000))\n",
        "    small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "    # Create dataloaders for iterating over the dataset\n",
        "    if args.debug_train:\n",
        "        train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=args.batch_size)\n",
        "        eval_dataloader = DataLoader(small_eval_dataset, batch_size=args.batch_size)\n",
        "        print(f\"Debug training...\")\n",
        "        print(f\"len(train_dataloader): {len(train_dataloader)}\")\n",
        "        print(f\"len(eval_dataloader): {len(eval_dataloader)}\")\n",
        "    else:\n",
        "        train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=args.batch_size)\n",
        "        eval_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=args.batch_size)\n",
        "        print(f\"Actual training...\")\n",
        "        print(f\"len(train_dataloader): {len(train_dataloader)}\")\n",
        "        print(f\"len(eval_dataloader): {len(eval_dataloader)}\")\n",
        "\n",
        "    # Train model on the original training dataset\n",
        "    if args.train:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "        model.to(device)\n",
        "        do_train(args, model, train_dataloader, save_dir=\"./out\")\n",
        "        # Change eval dir\n",
        "        args.model_dir = \"./out\"\n",
        "\n",
        "    # Train model on the augmented training dataset\n",
        "    if args.train_augmented:\n",
        "        train_dataloader = create_augmented_dataloader(args, dataset)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "        model.to(device)\n",
        "        do_train(args, model, train_dataloader, save_dir=\"./out_augmented\")\n",
        "        # Change eval dir\n",
        "        args.model_dir = \"./out_augmented\"\n",
        "\n",
        "    # Evaluate the trained model on the original test dataset\n",
        "    if args.eval:\n",
        "        out_file = os.path.basename(os.path.normpath(args.model_dir))\n",
        "        out_file = out_file + \"_original.txt\"\n",
        "        score = do_eval(eval_dataloader, args.model_dir, out_file)\n",
        "        print(\"Score: \", score)\n",
        "\n",
        "    # Evaluate the trained model on the transformed test dataset\n",
        "    if args.eval_transformed:\n",
        "        out_file = os.path.basename(os.path.normpath(args.model_dir))\n",
        "        out_file = out_file + \"_transformed.txt\"\n",
        "        eval_transformed_dataloader = create_transformed_dataloader(args, dataset, args.debug_transformation)\n",
        "        score = do_eval(eval_transformed_dataloader, args.model_dir, out_file)\n",
        "        print(\"Score: \", score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4-W5Rt7d4bS",
        "outputId": "fc91b41d-bc63-4431-93e3-87829d2c3752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "import random\n",
        "import argparse\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "def example_transform(example):\n",
        "    example[\"text\"] = example[\"text\"].lower()\n",
        "    return example\n",
        "\n",
        "\n",
        "### Rough guidelines --- typos\n",
        "# For typos, you can try to simulate nearest keys on the QWERTY keyboard for some of the letter (e.g. vowels)\n",
        "# You can randomly select each word with some fixed probability, and replace random letters in that word with one of the\n",
        "# nearest keys on the keyboard. You can vary the random probablity or which letters to use to achieve the desired accuracy.\n",
        "\n",
        "\n",
        "### Rough guidelines --- synonym replacement\n",
        "# For synonyms, use can rely on wordnet (already imported here). Wordnet (https://www.nltk.org/howto/wordnet.html) includes\n",
        "# something called synsets (which stands for synonymous words) and for each of them, lemmas() should give you a possible synonym word.\n",
        "# You can randomly select each word with some fixed probability to replace by a synonym.\n",
        "\n",
        "\n",
        "def custom_transform(example):\n",
        "    ################################\n",
        "    ##### YOUR CODE BEGINGS HERE ###\n",
        "\n",
        "    # Design and implement the transformation as mentioned in pdf\n",
        "    # You are free to implement any transformation but the comments at the top roughly describe\n",
        "    # how you could implement two of them --- synonym replacement and typos.\n",
        "\n",
        "    # You should update example[\"text\"] using your transformation\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE ######\n",
        "\n",
        "    return example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0NJ35YKeC7O",
        "outputId": "e1034b7f-1837-4158-9c48-10076c16f060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-15 03:04:50.437941: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 03:04:50.455528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763175890.476832    2466 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763175890.483245    2466 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763175890.499451    2466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763175890.499479    2466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763175890.499482    2466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763175890.499485    2466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 03:04:50.504460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 354kB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 4.55MB/s]\n",
            "vocab.txt: 100% 213k/213k [00:00<00:00, 52.4MB/s]\n",
            "tokenizer.json: 100% 436k/436k [00:00<00:00, 27.8MB/s]\n",
            "README.md: 7.81kB [00:00, 35.9MB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 21.0M/21.0M [00:01<00:00, 15.2MB/s]\n",
            "plain_text/test-00000-of-00001.parquet: 100% 20.5M/20.5M [00:00<00:00, 29.0MB/s]\n",
            "plain_text/unsupervised-00000-of-00001.p(…): 100% 42.0M/42.0M [00:00<00:00, 47.7MB/s]\n",
            "Generating train split: 100% 25000/25000 [00:00<00:00, 138547.32 examples/s]\n",
            "Generating test split: 100% 25000/25000 [00:00<00:00, 209537.93 examples/s]\n",
            "Generating unsupervised split: 100% 50000/50000 [00:00<00:00, 176706.59 examples/s]\n",
            "Map: 100% 25000/25000 [00:09<00:00, 2610.43 examples/s]\n",
            "Map: 100% 25000/25000 [00:09<00:00, 2565.88 examples/s]\n",
            "Map: 100% 50000/50000 [00:19<00:00, 2535.71 examples/s]\n",
            "Debug training...\n",
            "len(train_dataloader): 500\n",
            "len(eval_dataloader): 125\n",
            "model.safetensors: 100% 436M/436M [00:01<00:00, 331MB/s]\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1500/1500 [04:01<00:00,  6.25it/s]Training completed...\n",
            "Saving Model....\n",
            "100% 1500/1500 [04:02<00:00,  6.19it/s]\n",
            "Downloading builder script: 4.20kB [00:00, 17.6MB/s]\n",
            "100% 125/125 [00:06<00:00, 19.05it/s]\n",
            "Score:  {'accuracy': 0.894}\n"
          ]
        }
      ],
      "source": [
        "!python main.py --train --eval --debug_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7aSSYS_eM4B",
        "outputId": "444b3ee7-85a3-43c0-fc2f-671e23276c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "2025-11-15 18:52:34.611221: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 18:52:34.630510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763232754.652966    2286 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763232754.659625    2286 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763232754.677122    2286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763232754.677159    2286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763232754.677162    2286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763232754.677165    2286 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 18:52:34.684241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 287kB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 3.28MB/s]\n",
            "vocab.txt: 100% 213k/213k [00:00<00:00, 1.30MB/s]\n",
            "tokenizer.json: 100% 436k/436k [00:00<00:00, 2.61MB/s]\n",
            "README.md: 7.81kB [00:00, 30.9MB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 21.0M/21.0M [00:00<00:00, 32.0MB/s]\n",
            "plain_text/test-00000-of-00001.parquet: 100% 20.5M/20.5M [00:00<00:00, 65.8MB/s]\n",
            "plain_text/unsupervised-00000-of-00001.p(…): 100% 42.0M/42.0M [00:00<00:00, 112MB/s] \n",
            "Generating train split: 100% 25000/25000 [00:00<00:00, 140000.35 examples/s]\n",
            "Generating test split: 100% 25000/25000 [00:00<00:00, 192173.24 examples/s]\n",
            "Generating unsupervised split: 100% 50000/50000 [00:00<00:00, 191671.09 examples/s]\n",
            "Map: 100% 25000/25000 [00:10<00:00, 2484.98 examples/s]\n",
            "Map: 100% 25000/25000 [00:09<00:00, 2515.76 examples/s]\n",
            "Map: 100% 50000/50000 [00:19<00:00, 2534.75 examples/s]\n",
            "Actual training...\n",
            "len(train_dataloader): 3125\n",
            "len(eval_dataloader): 3125\n",
            "model.safetensors: 100% 436M/436M [00:01<00:00, 392MB/s]\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 9375/9375 [24:40<00:00,  6.28it/s]Training completed...\n",
            "Saving Model....\n",
            "100% 9375/9375 [24:41<00:00,  6.33it/s]\n",
            "Downloading builder script: 4.20kB [00:00, 15.0MB/s]\n",
            "100% 3125/3125 [02:41<00:00, 19.38it/s]\n",
            "Score:  {'accuracy': 0.93032}\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-1-code/\n",
        "\n",
        "!python main.py --train --eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "riFPh6ejfn9S",
        "outputId": "fcaf802b-144c-4855-fda1-bb747b418ea4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_92dceccb-bda2-4b0e-8ad0-2e97f23d4ebe\", \"out_original.txt\", 100000)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Download output file\n",
        "from google.colab import files\n",
        "\n",
        "# Download the result file for submission\n",
        "files.download('/content/hw4/part-1-code/out_original.txt')\n",
        "\n",
        "# Also save to Google Drive (optional but recommended)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/hw4/part-1-code/out_original.txt /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y35Ts7HivWwB",
        "outputId": "566436c9-8b14-4fa7-d664-0c46e2d51f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "import random\n",
        "import argparse\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "def example_transform(example):\n",
        "    example[\"text\"] = example[\"text\"].lower()\n",
        "    return example\n",
        "\n",
        "\n",
        "### Rough guidelines --- typos\n",
        "# For typos, you can try to simulate nearest keys on the QWERTY keyboard for some of the letter (e.g. vowels)\n",
        "# You can randomly select each word with some fixed probability, and replace random letters in that word with one of the\n",
        "# nearest keys on the keyboard. You can vary the random probablity or which letters to use to achieve the desired accuracy.\n",
        "\n",
        "\n",
        "### Rough guidelines --- synonym replacement\n",
        "# For synonyms, use can rely on wordnet (already imported here). Wordnet (https://www.nltk.org/howto/wordnet.html) includes\n",
        "# something called synsets (which stands for synonymous words) and for each of them, lemmas() should give you a possible synonym\n",
        "\n",
        "# You can randomly select each word with some fixed probability to replace by a synonym.\n",
        "\n",
        "\n",
        "def custom_transform(example):\n",
        "    ################################\n",
        "    ##### YOUR CODE BEGINGS HERE ###\n",
        "\n",
        "    # Transformation: Synonym Replacement using WordNet\n",
        "    #\n",
        "    # Strategy: Randomly replace words with their synonyms from WordNet.\n",
        "    # For each word in the sentence, with probability 0.3, we replace it\n",
        "    # with a random synonym if one exists in WordNet.\n",
        "    #\n",
        "    # This is reasonable because users naturally use different words to\n",
        "    # express the same sentiment (e.g., \"great\" vs \"excellent\",\n",
        "    # \"movie\" vs \"film\", \"bad\" vs \"terrible\").\n",
        "\n",
        "    text = example[\"text\"]\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        # Replace with 30% probability\n",
        "        if random.random() < 0.3:\n",
        "            # Get synonyms from WordNet\n",
        "            synsets = wordnet.synsets(word)\n",
        "            if synsets:\n",
        "                # Get all lemmas (synonym words) from all synsets\n",
        "                lemmas = []\n",
        "                for syn in synsets:\n",
        "                    for lemma in syn.lemmas():\n",
        "                        lemma_name = lemma.name().replace('_', ' ')\n",
        "                        if lemma_name.lower() != word.lower():\n",
        "                            lemmas.append(lemma_name)\n",
        "\n",
        "                # Replace with random synonym if available\n",
        "                if lemmas:\n",
        "                    new_words.append(random.choice(lemmas))\n",
        "                else:\n",
        "                    new_words.append(word)\n",
        "            else:\n",
        "                new_words.append(word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    # Reconstruct the sentence\n",
        "    detokenizer = TreebankWordDetokenizer()\n",
        "    example[\"text\"] = detokenizer.detokenize(new_words)\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE ######\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5bV-5Ubss0o",
        "outputId": "f92bac46-d5b4-4bde-c016-e5e67e6a0a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "2025-11-15 18:16:32.591301: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 18:16:32.609551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763230592.631260    2936 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763230592.637933    2936 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763230592.655058    2936 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763230592.655099    2936 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763230592.655102    2936 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763230592.655105    2936 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 18:16:32.660182: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Map: 100% 25000/25000 [00:09<00:00, 2644.49 examples/s]\n",
            "Actual training...\n",
            "len(train_dataloader): 3125\n",
            "len(eval_dataloader): 3125\n",
            "Map: 100% 5/5 [00:03<00:00,  1.61 examples/s]\n",
            "Original Example  0\n",
            "{'text': \"<br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\", 'label': 1}\n",
            "\n",
            "\n",
            "Transformed Example  0\n",
            "{'text': \"<br /> <br /> When I unsuspectedly rented angstrom unit Thousand Akka, I thought I was in for an harbor King Lear tarradiddle and of course of study Michelle Pfeiffer was in it, so what could go haywire? <br /> <br /> Very quickly, however, I realized that this story was about antiophthalmic factor Thousand former Things too just Acres . I started crying and couldn't stop until long after the movie ended . Thank you Jane, Laura and Jocelyn, for bringing the States such adenine wonderfully elusive and feel for moving-picture show! Thank you cast, for be involved and portraying the characters with such depth and softness! <br /> <Red Brigades /> I recognized the furious sister; the Runaway baby and the sister in Denial . I recognized the opprobrious Husband and why he be there and and so the Father, Buckeye State oh the Father...completely wondrous played . I also recognized myself and this movie comprise Associate in Nursing eye-opener, a relief, deoxyadenosine monophosphate chance to face my OWN Sojourner Truth and finally make something about it . I really hope A one thousand Acres has possess the same effect along some others forbidden there. <br /> <bromine /> Since I didn't understand why the cover said the film was around sisters fight ended land -they weren't struggle from each one other at all- one watched it a second time . so unity was able to see that if one hadn't lived a similar story, one would easy girl the overwhelming undertide of dread and care and the deep bond between the sisters that runs through it totally . That is exactly the reason out why people Hoosier State general ofttimes overlook the truth about their neighbors for instance. <br /> <br /> But yet another grounds why this movie be so perfect! <br /> <br /> I don't hold a rat's ass (pardon my French) about to what extend the King Lear story be followed . All I bed is that I john honestly enounce: this picture show take changed my life. <br /> <Brigate Rosse /> Keep upwardly the good work guys, you CAN and DO make a difference. <br /> <br />\", 'label': 1}\n",
            "==============================\n",
            "Original Example  1\n",
            "{'text': \"This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight films (none of which was ever released in the U.S.). 'O.S.S.117:Cairo,Nest Of Spies' is a breezy little comedy that should not...repeat NOT, be taken too seriously. Our protagonist finds himself in the middle of a spy chase in Egypt (with Morroco doing stand in for Egypt) to find out about a long lost friend. What follows is the standard James Bond/Inspector Cloussou kind of antics. Although our man is something of an overt xenophobe,sexist,homophobe, it's treated as pure farce (as I said, don't take it too seriously). Although there is a bit of rough language & cartoon violence, it's basically okay for older kids (ages 12 & up). As previously stated in the subject line, just sit back,pass the popcorn & just enjoy.\", 'label': 1}\n",
            "\n",
            "\n",
            "Transformed Example  1\n",
            "{'text': \"This is the latest entrance in the long series of film with the French agent, O.S.S . 117 (the French answer to jampack Bond). The serial was launched Indiana the early 1950's, and spawned astatine least eighter from Decatur films (none of which was ever let go of inwards the U.S.).' O.S.S.117: El Qahira, Nest Of Spies' is angstrom unit windy footling comedy that should non...repeat non, be taken to a fault seriously . Our protagonist finds himself in the middle of a spy chase in United Arab Republic (with Morroco do stand in for Egypt) to find taboo about a long lost protagonist . What follows is the monetary standard James Bond/Inspector Cloussou kind of antics . Although our man be something of an overt xenophobe, male chauvinist, homophobe, information technology's treated as pure travesty (as atomic number 53 said, don't take it too seriously). Although at that place is a burn of rough language & cartoon violence, it's essentially o.k. for older kids (ages 12 & upwards). As previously stated in the submit railway line, just sit back, pass the popcorn & just relish.\", 'label': 1}\n",
            "==============================\n",
            "Original Example  2\n",
            "{'text': 'This movie was so frustrating. Everything seemed energetic and I was totally prepared to have a good time. I at least thought I\\'d be able to stand it. But, I was wrong. First, the weird looping? It was like watching \"America\\'s Funniest Home Videos\". The damn parents. I hated them so much. The stereo-typical Latino family? I need to speak with the person responsible for this. We need to have a talk. That little girl who was always hanging on someone? I just hated her and had to mention it. Now, the final scene transcends, I must say. It\\'s so gloriously bad and full of badness that it is a movie of its own. What crappy dancing. Horrible and beautiful at once.', 'label': 0}\n",
            "\n",
            "\n",
            "Transformed Example  2\n",
            "{'text': 'This movie was so frustrate . Everything seemed energetic and I was totally prepared to give a good time . I at least thought I\\'d be able to stand it . just, ace personify damage . First, the weird looping? information technology was like watching \"United States\\'s funny Home Videos\". The damn parents . I hat them therefore much . The stereo-typical Latino family? atomic number 53 need to speak with the person responsible for this . We need to have a talk . That petty girl who was always wall hanging along mortal? ane hardly hated her and get to bring up information technology . Now, the final scene exceed, I must allege . information technology\\'s thusly gloriously bad and full of badness that it is a movie of information technology own . What lousy dancing . Horrible and beautiful at once.', 'label': 0}\n",
            "==============================\n",
            "Original Example  3\n",
            "{'text': 'I was truly and wonderfully surprised at \"O\\' Brother, Where Art Thou?\" The video store was out of all the movies I was planning on renting, so then I came across this. I came home and as I watched I became engrossed and found myself laughing out loud. The Coen\\'s have made a magnificiant film again. But I think the first time you watch this movie, you get to know the characters. The second time, now that you know them, you laugh sooo hard it could hurt you. I strongly would reccomend ANYONE seeing this because if you are not, you are truly missing a film gem for the ages. 10/10', 'label': 1}\n",
            "\n",
            "\n",
            "Transformed Example  3\n",
            "{'text': 'single was truly and wonderfully surprise at \"group O\\' Brother, Where fine art Thou?\" The picture stock personify out of all the moving picture I equal plan on renting, then then I came across this . single derive interior and as one watched I become engrossed and found myself laughing out loud . The Coen\\'s have make a magnificiant take again . But I think the world-class time you watch this movie, you get to know the characters . The second time, now that you know them, you jape sooo hard it could ache you . ane strongly would reccomend ANYONE seeing this because if you are not, you are truly missing a moving picture gem for the age . 10/10', 'label': 1}\n",
            "==============================\n",
            "Original Example  4\n",
            "{'text': 'This movie spends most of its time preaching that it is the script that makes the movie, but apparently there was no script when they shot this waste of time! The trailer makes this out to be a comedy, but the film can\\'t decide if it wants to be a comedy, a drama, a romance or an action film. Press releases indicated that Shatner and Hamlin made this movie because they loved the script (what were they thinking?). If you like William Shatner (I do) see \"Free Enterprise\" instead.', 'label': 0}\n",
            "\n",
            "\n",
            "Transformed Example  4\n",
            "{'text': 'This moving-picture show expend almost of information technology time preaching that it is the script that makes the motion picture, but apparently on that point was no script when they shot this waste of time! The trailer make this out to constitute a comedy, simply the film can\\'t decide if it wants to be a drollery, a drama, a romance or Associate in Nursing action film . Press releases indicated that Shatner and Hamlin made this movie because they loved the book (what equal they thinking?). If you like William Shatner (I fare) see \"dislodge Enterprise\" instead.', 'label': 0}\n",
            "==============================\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-1-code/\n",
        "\n",
        "!python main.py --eval_transformed --debug_transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrCBjG37s-9D",
        "outputId": "3cde8390-ff98-4c3c-9087-f627aa993c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "2025-11-15 19:26:36.389253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 19:26:36.407481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763234796.429402   10925 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763234796.436103   10925 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763234796.452815   10925 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234796.452846   10925 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234796.452849   10925 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763234796.452853   10925 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 19:26:36.457753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Map: 100% 25000/25000 [00:09<00:00, 2624.29 examples/s]\n",
            "Actual training...\n",
            "len(train_dataloader): 3125\n",
            "len(eval_dataloader): 3125\n",
            "Map: 100% 25000/25000 [01:28<00:00, 281.17 examples/s]\n",
            "Map: 100% 25000/25000 [00:09<00:00, 2513.46 examples/s]\n",
            "100% 3125/3125 [02:41<00:00, 19.32it/s]\n",
            "Score:  {'accuracy': 0.88496}\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-1-code/\n",
        "\n",
        "!python main.py --eval_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4QLCliQvK-q",
        "outputId": "81d0dd28-8884-4500-ed69-0272400c7ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "2025-11-15 19:56:36.025262: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 19:56:36.043314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763236596.065158   18603 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763236596.071785   18603 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763236596.088698   18603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763236596.088726   18603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763236596.088729   18603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763236596.088732   18603 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 19:56:36.093736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Map: 100% 50000/50000 [00:19<00:00, 2558.59 examples/s]\n",
            "Actual training...\n",
            "len(train_dataloader): 3125\n",
            "len(eval_dataloader): 3125\n",
            "Map: 50000 examples [01:28, 282.03 examples/s]\n",
            "Map: 50000 examples [00:10, 2343.55 examples/s]\n",
            "100% 3125/3125 [02:41<00:00, 19.31it/s]\n",
            "Score:  {'accuracy': 0.88496}\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-1-code/\n",
        "\n",
        "!python main.py --eval_transformed --model_dir out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhObmYVV0SI3",
        "outputId": "67ab3db3-3e52-4eb4-bbf8-0fd31e420168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 136\n",
            "drwxr-xr-x 3 root root   4096 Nov 15 18:43 .\n",
            "drwxr-xr-x 3 root root   4096 Nov 15 18:08 ..\n",
            "-rw-r--r-- 1 root root   9258 Nov 15 18:36 main.py\n",
            "-rw-r--r-- 1 root root 100000 Nov 15 18:40 out_original.txt\n",
            "drwxr-xr-x 2 root root   4096 Nov 15 18:14 __pycache__\n",
            "-rw-r--r-- 1 root root    383 Nov 15 18:09 README.md\n",
            "-rw-r--r-- 1 root root     98 Nov 15 18:09 requirements.txt\n",
            "-rw-r--r-- 1 root root   3077 Nov 15 18:14 utils.py\n"
          ]
        }
      ],
      "source": [
        "!ls -la /content/hw4/part-1-code/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itFDiKtb1Alz",
        "outputId": "37cdd887-e1fa-44cb-e312-f022991a2597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5EOHD3G1QUd",
        "outputId": "0962c45d-ac77-4c8b-e08c-b363ec943fb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 4082\n",
            "-rw------- 1 root root 282383 May  4  2025 'Adapted CLIP Model Training and Testing'\n",
            "drwx------ 2 root root   4096 Mar 10  2025 'Colab Notebooks'\n",
            "-rw------- 1 root root    169 Jun  4 19:11  Companies.gsheet\n",
            "-rw------- 1 root root    169 Sep 25 19:19 'given this is an assignment question we are doing....gsheet'\n",
            "drwx------ 2 root root   4096 Mar 26  2025 'Google AI Studio'\n",
            "-rw------- 1 root root    169 Sep 18 04:02  Homework1_ComputerVision_Fall2025.gdoc\n",
            "drwx------ 3 root root   4096 Nov 15 02:57  hw4\n",
            "-rw------- 1 root root 776307 Jun  2 17:16  M2Lines_Coding_Assignment.ipynb\n",
            "-rw------- 1 root root    169 Dec  2  2024 'ML project presentation-5.gslides'\n",
            "-rw------- 1 root root    169 May 20 20:53 'nyu cs.gsheet'\n",
            "-rw------- 1 root root   3330 Jan  6  2025  NYU_SR_TUNF.pdf\n",
            "-rw------- 1 root root  35978 Nov  3 21:44 'NYU_transcript_3 (1).pdf'\n",
            "-rw------- 1 root root  35978 Nov  3 21:44 'NYU_transcript_3 (2).pdf'\n",
            "-rw------- 1 root root  35978 Nov  3 21:44 'NYU_transcript_3 (3).pdf'\n",
            "-rw------- 1 root root  35978 Nov  3 21:44  NYU_transcript_3.pdf\n",
            "-rw------- 1 root root 100000 Nov 15 04:20  out_original.txt\n",
            "-rw------- 1 root root    169 Dec  1  2024  projectml.gdoc\n",
            "-rw------- 1 root root    169 Oct 21 18:40 'QA Associate - Practical Interview - Raj Trikha.gsheet'\n",
            "-rw------- 1 root root 173381 May 19 01:32  Raj_Trikha.zip\n",
            "-rw------- 1 root root 170365 Sep 14 17:22  RTai23.pdf\n",
            "-rw------- 1 root root 168884 Oct 14 22:45 'RTair1 (1).pdf'\n",
            "-rw------- 1 root root 169077 Oct 14 22:59  RTair1.pdf\n",
            "-rw------- 1 root root 165266 Jun 23 14:56  RTdl2.pdf\n",
            "-rw------- 1 root root 157357 Mar  5  2025  RTf3.pdf\n",
            "-rw------- 1 root root 165457 Nov  3 21:42  RTnlp.pdf\n",
            "-rw------- 1 root root 197087 Mar  9  2025  RTOncamp0_merged-2.pdf\n",
            "-rw------- 1 root root 154378 Mar  9  2025  RTOncamp0.pdf\n",
            "-rw------- 1 root root 160600 Sep  3 02:12 'RTOncamp10 (1).pdf'\n",
            "-rw------- 1 root root 160600 Sep  3 02:12  RTOncamp10.pdf\n",
            "-rw------- 1 root root 158263 Dec 20  2024  RToriginal.pdf\n",
            "-rw------- 1 root root 166206 May 14  2025  RTp4.pdf\n",
            "-rw------- 1 root root 684767 Aug 18 17:06 'Screenshot 2025-08-18 at 1.06.00 PM.png'\n",
            "-rw------- 1 root root    169 Nov 30  2024 'Untitled document.gdoc'\n"
          ]
        }
      ],
      "source": [
        "!ls -la /content/drive/MyDrive/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Kk0KMX1eXe",
        "outputId": "14fc0cb5-a50e-42fe-dbca-082dd7b1d530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/out_original.txt\n"
          ]
        }
      ],
      "source": [
        "!find /content/drive/MyDrive/ -name \"out*\" -o -name \"*bert*\" -o -name \"*.safetensors\" 2>/dev/null | head -20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ubJWyIEf2Uvh"
      },
      "outputs": [],
      "source": [
        "!find /content/drive/MyDrive/ -name \"config.json\" 2>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "bBWBsE652Y9S"
      },
      "outputs": [],
      "source": [
        "!find /content/drive/MyDrive/ -type d -name \"out\" 2>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQMePxkT2bPt",
        "outputId": "4ea8ba5e-a832-47ae-8f4a-8a9838c8ebd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 423128\n",
            "drwxr-xr-x 2 root root      4096 Nov 15 19:18 .\n",
            "drwxr-xr-x 4 root root      4096 Nov 15 19:28 ..\n",
            "-rw-r--r-- 1 root root       681 Nov 15 19:18 config.json\n",
            "-rw-r--r-- 1 root root 433270768 Nov 15 19:18 model.safetensors\n"
          ]
        }
      ],
      "source": [
        "  !ls -la /content/hw4/part-1-code/out/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FCG0TrKrFg7M",
        "outputId": "f79e9d73-91ff-4dbb-b580-e0117ca41e1c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5bc547f3-a03c-4110-b7eb-aebac51fed75\", \"out_transformed.txt\", 100000)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Download the output file for submission\n",
        "from google.colab import files\n",
        "files.download('out_transformed.txt')\n",
        "\n",
        "# Save everything to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/hw4/part-1-code/out /content/drive/MyDrive/hw4_bert_model\n",
        "!cp /content/hw4/part-1-code/out_transformed.txt /content/drive/MyDrive/\n",
        "!cp /content/hw4/part-1-code/out_original.txt /content/drive/MyDrive/\n",
        "!cp /content/hw4/part-1-code/utils.py /content/drive/MyDrive/\n",
        "!cp /content/hw4/part-1-code/main.py /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FsnoWa0HUeQ",
        "outputId": "7aaaaf6a-de4a-4603-f6f0-f71006cd250b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "import random\n",
        "import argparse\n",
        "from utils import *\n",
        "import os\n",
        "\n",
        "# Set seed\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# Tokenize the input\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "# Core training function\n",
        "def do_train(args, model, train_dataloader, save_dir=\"./out\"):\n",
        "    optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
        "    num_epochs = args.num_epochs\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        "    )\n",
        "    model.train()\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "    ################################\n",
        "    ##### YOUR CODE BEGINGS HERE ###\n",
        "\n",
        "    # Implement the training loop --- make sure to use the optimizer and lr_sceduler (learning rate scheduler)\n",
        "    # Remember that pytorch uses gradient accumumlation so you need to use zero_grad (https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html)\n",
        "    # You can use progress_bar.update(1) to see the progress during training\n",
        "    # You can refer to the pytorch tutorial covered in class for reference\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_dataloader:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            # Zero gradients for next iteration\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE ######\n",
        "\n",
        "    print(\"Training completed...\")\n",
        "    print(\"Saving Model....\")\n",
        "    model.save_pretrained(save_dir)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "# Core evaluation function\n",
        "def do_eval(eval_dataloader, output_dir, out_file):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    out_file = open(out_file, \"w\")\n",
        "\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "        # write to output file\n",
        "        for pred, label in zip(predictions, batch[\"labels\"]):\n",
        "                out_file.write(f\"{pred.item()}\\n\")\n",
        "                out_file.write(f\"{label.item()}\\n\")\n",
        "    out_file.close()\n",
        "    score = metric.compute()\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "# Created a dataladoer for the augmented training dataset\n",
        "def create_augmented_dataloader(args, dataset):\n",
        "    ################################\n",
        "    ##### YOUR CODE BEGINGS HERE ###\n",
        "\n",
        "    # Here, 'dataset' is the original dataset. You should return a dataloader called 'train_dataloader' -- this\n",
        "    # dataloader will be for the original training split augmented with 5k random transformed examples from the training set.\n",
        "    # You may find it helpful to see how the dataloader was created at other place in this code.\n",
        "\n",
        "    # Step 1: Get the original training dataset\n",
        "    train_dataset = dataset[\"train\"]\n",
        "\n",
        "    # Step 2: Sample 5000 random examples from training set\n",
        "    # Shuffle and select 5000 examples\n",
        "    sampled_dataset = train_dataset.shuffle(seed=42).select(range(5000))\n",
        "\n",
        "    # Step 3: Apply transformation to these 5000 examples\n",
        "    transformed_dataset = sampled_dataset.map(custom_transform, load_from_cache_file=False)\n",
        "\n",
        "    # Step 4: Combine original training data with transformed examples\n",
        "    from datasets import concatenate_datasets\n",
        "    augmented_dataset = concatenate_datasets([train_dataset, transformed_dataset])\n",
        "\n",
        "    # Step 5: Tokenize the augmented dataset\n",
        "    augmented_tokenized = augmented_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    augmented_tokenized = augmented_tokenized.remove_columns([\"text\"])\n",
        "    augmented_tokenized = augmented_tokenized.rename_column(\"label\", \"labels\")\n",
        "    augmented_tokenized.set_format(\"torch\")\n",
        "\n",
        "    # Step 6: Create dataloader\n",
        "    train_dataloader = DataLoader(augmented_tokenized, shuffle=True, batch_size=args.batch_size)\n",
        "\n",
        "    ##### YOUR CODE ENDS HERE ######\n",
        "\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "\n",
        "# Create a dataloader for the transformed test set\n",
        "def create_transformed_dataloader(args, dataset, debug_transformation):\n",
        "    # Print 5 random transformed examples\n",
        "    if debug_transformation:\n",
        "        small_dataset = dataset[\"test\"].shuffle(seed=42).select(range(5))\n",
        "        small_transformed_dataset = small_dataset.map(custom_transform, load_from_cache_file=False)\n",
        "        for k in range(5):\n",
        "            print(\"Original Example \", str(k))\n",
        "            print(small_dataset[k])\n",
        "            print(\"\\n\")\n",
        "            print(\"Transformed Example \", str(k))\n",
        "            print(small_transformed_dataset[k])\n",
        "            print('=' * 30)\n",
        "\n",
        "        exit()\n",
        "\n",
        "    transformed_dataset = dataset[\"test\"].map(custom_transform, load_from_cache_file=False)\n",
        "    transformed_tokenized_dataset = transformed_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    transformed_tokenized_dataset = transformed_tokenized_dataset.remove_columns([\"text\"])\n",
        "    transformed_tokenized_dataset = transformed_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "    transformed_tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "    transformed_val_dataset = transformed_tokenized_dataset\n",
        "    eval_dataloader = DataLoader(transformed_val_dataset, batch_size=args.batch_size)\n",
        "\n",
        "    return eval_dataloader\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Arguments\n",
        "    parser.add_argument(\"--train\", action=\"store_true\", help=\"train a model on the training data\")\n",
        "    parser.add_argument(\"--train_augmented\", action=\"store_true\", help=\"train a model on the augmented training data\")\n",
        "    parser.add_argument(\"--eval\", action=\"store_true\", help=\"evaluate model on the test set\")\n",
        "    parser.add_argument(\"--eval_transformed\", action=\"store_true\", help=\"evaluate model on the transformed test set\")\n",
        "    parser.add_argument(\"--model_dir\", type=str, default=\"./out\")\n",
        "    parser.add_argument(\"--debug_train\", action=\"store_true\",\n",
        "                        help=\"use a subset for training to debug your training loop\")\n",
        "    parser.add_argument(\"--debug_transformation\", action=\"store_true\",\n",
        "                        help=\"print a few transformed examples for debugging\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
        "    parser.add_argument(\"--num_epochs\", type=int, default=3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    global device\n",
        "    global tokenizer\n",
        "\n",
        "    # Device\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Prepare dataset for use by model\n",
        "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "    tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "    small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(4000))\n",
        "    small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "    # Create dataloaders for iterating over the dataset\n",
        "    if args.debug_train:\n",
        "        train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=args.batch_size)\n",
        "        eval_dataloader = DataLoader(small_eval_dataset, batch_size=args.batch_size)\n",
        "        print(f\"Debug training...\")\n",
        "        print(f\"len(train_dataloader): {len(train_dataloader)}\")\n",
        "        print(f\"len(eval_dataloader): {len(eval_dataloader)}\")\n",
        "    else:\n",
        "        train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=args.batch_size)\n",
        "        eval_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=args.batch_size)\n",
        "        print(f\"Actual training...\")\n",
        "        print(f\"len(train_dataloader): {len(train_dataloader)}\")\n",
        "        print(f\"len(eval_dataloader): {len(eval_dataloader)}\")\n",
        "\n",
        "    # Train model on the original training dataset\n",
        "    if args.train:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "        model.to(device)\n",
        "        do_train(args, model, train_dataloader, save_dir=\"./out\")\n",
        "        # Change eval dir\n",
        "        args.model_dir = \"./out\"\n",
        "\n",
        "    # Train model on the augmented training dataset\n",
        "    if args.train_augmented:\n",
        "        train_dataloader = create_augmented_dataloader(args, dataset)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "        model.to(device)\n",
        "        do_train(args, model, train_dataloader, save_dir=\"./out_augmented\")\n",
        "        # Change eval dir\n",
        "        args.model_dir = \"./out_augmented\"\n",
        "\n",
        "    # Evaluate the trained model on the original test dataset\n",
        "    if args.eval:\n",
        "        out_file = os.path.basename(os.path.normpath(args.model_dir))\n",
        "        out_file = out_file + \"_original.txt\"\n",
        "        score = do_eval(eval_dataloader, args.model_dir, out_file)\n",
        "        print(\"Score: \", score)\n",
        "\n",
        "    # Evaluate the trained model on the transformed test dataset\n",
        "    if args.eval_transformed:\n",
        "        out_file = os.path.basename(os.path.normpath(args.model_dir))\n",
        "        out_file = out_file + \"_transformed.txt\"\n",
        "        eval_transformed_dataloader = create_transformed_dataloader(args, dataset, args.debug_transformation)\n",
        "        score = do_eval(eval_transformed_dataloader, args.model_dir, out_file)\n",
        "        print(\"Score: \", score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8A6rH1VJBUF",
        "outputId": "5aad2c9c-d455-44e5-9798-40db28ba2334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "2025-11-15 20:11:04.499852: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 20:11:04.517577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763237464.539006   22480 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763237464.545474   22480 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763237464.561943   22480 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763237464.561973   22480 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763237464.561976   22480 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763237464.561979   22480 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 20:11:04.566885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Actual training...\n",
            "len(train_dataloader): 3125\n",
            "len(eval_dataloader): 3125\n",
            "Map: 100% 5000/5000 [00:22<00:00, 223.95 examples/s]\n",
            "Map: 100% 30000/30000 [00:12<00:00, 2438.25 examples/s]\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 11250/11250 [29:35<00:00,  6.34it/s]Training completed...\n",
            "Saving Model....\n",
            "100% 11250/11250 [29:36<00:00,  6.33it/s]\n",
            "Map: 50000 examples [01:22, 304.08 examples/s]\n",
            "Map: 50000 examples [00:10, 2341.60 examples/s]\n",
            "100% 3125/3125 [02:41<00:00, 19.32it/s]\n",
            "Score:  {'accuracy': 0.8954}\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-1-code/\n",
        "\n",
        "!python main.py --train_augmented --eval_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IRktoG8JHD_",
        "outputId": "18eee6ac-8023-449a-dae1-b477c8df22b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-15 20:46:34.670788: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 20:46:34.688477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763239594.709834   31435 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763239594.716271   31435 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763239594.732561   31435 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763239594.732590   31435 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763239594.732593   31435 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763239594.732595   31435 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 20:46:34.737541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Actual training...\n",
            "len(train_dataloader): 3125\n",
            "len(eval_dataloader): 3125\n",
            "100% 3125/3125 [02:41<00:00, 19.31it/s]\n",
            "Score:  {'accuracy': 0.91948}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the augmented model on original test set (~5 min)\n",
        "!python main.py --eval --model_dir out_augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Y7zQp1J2RPHx",
        "outputId": "47291c98-f3d9-4de5-d5fc-4c2d757c19df"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_31d19714-2f2e-46fc-9219-d5162e32ed42\", \"out_augmented_original.txt\", 100000)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5a89b72e-641b-418e-85ed-a2cd0fe68208\", \"out_augmented_transformed.txt\", 100000)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Download submission files\n",
        "from google.colab import files\n",
        "\n",
        "files.download('out_augmented_original.txt')\n",
        "files.download('out_augmented_transformed.txt')\n",
        "\n",
        "# Save everything to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/hw4/part-1-code/out_augmented /content/drive/MyDrive/hw4_bert_augmented_model\n",
        "!cp out_augmented_original.txt /content/drive/MyDrive/\n",
        "!cp out_augmented_transformed.txt /content/drive/MyDrive/\n",
        "!cp main.py /content/drive/MyDrive/\n",
        "!cp utils.py /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJJOYAexST-f",
        "outputId": "3ebc081d-4880-4212-835e-bab22dcd091a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "  %cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bC1RkskUXecY"
      },
      "outputs": [],
      "source": [
        "  # Create directory and upload Part 2 files\n",
        "  !mkdir -p /content/hw4/part-2-code/data\n",
        "  !mkdir -p /content/hw4/part-2-code/records\n",
        "  !mkdir -p /content/hw4/part-2-code/results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gapvJtNOXhTr",
        "outputId": "e27af004-035c-4327-ea73-ca1158f65986"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "  !pip install transformers torch nltk datasets -q\n",
        "\n",
        "  import nltk\n",
        "  nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d7969b8760a445deaa75c3feee13d296",
            "4faebdf85df74ca6b36e0c0f73ed1196",
            "b1a84a81c8cc4eaea548c12817431975",
            "3665334ffc344048afec8f4c5a8a8d6a",
            "eb73559f9cc545469692d12be53c621e",
            "ab9f1234743f45c8abae4acb98c3146f",
            "ba3e850762294fb8bc440cd92823d125",
            "c3c56b9250bd475babc5cc8db77e2abd",
            "84aea06d2222454391a3ca591f4605b0",
            "508d6f45c20f43ac9edcc4119eaec027",
            "37124f01b2144e968ddae9dae4f89943",
            "5e2e870ec24948ebacdfbf1defe25a2f",
            "6333b3aec7ba4a229efab525b20345e9",
            "efb1aba2bc6f4b57ad2d87d35497d15e",
            "9418fb2859bd4db7881ec0ea70d6601b",
            "732305ee47ce423e8c2e65a7f828ab1f",
            "2279912a52a8434bbf96d885c5ca8e34",
            "1c8c915477c44a159be3154af507bcd8",
            "60ac414cc5ee47d8aadded2868378a77",
            "e51ca5eaa7a0413bb4e1b2f5f09df68b",
            "ae7ea71dcbb44254ac220f01deebd599",
            "15607f73ed5c42f3a9da18b59b438bad",
            "5259c41d265f4c18aed1ca589ddf56d7",
            "a1f810ba555d495fbda97d447f7d2f12",
            "f5ca95646d454852809e6b54e5ecc904",
            "5fbbcbc6314e486dbf15d6790120c36e",
            "050df5f2dd4b43beada32764980efe07",
            "2d3e7c24339c4a56abb7e9330693398f",
            "a770bb39042b470aac9f41af48f50057",
            "df236049975c4e14b2d5967f7d571703",
            "fadf7fe651b04843b010b23a94559669",
            "4be5589f52244fc09c649e74aa7ab4e9",
            "484fc1a74e9740a7aa7d74d5f29fb969"
          ]
        },
        "id": "vYm8NXWnXldR",
        "outputId": "800bbe57-94a4-4120-8e93-3bcf6dc9e5b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7969b8760a445deaa75c3feee13d296",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e2e870ec24948ebacdfbf1defe25a2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5259c41d265f4c18aed1ca589ddf56d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TABLE 1: BEFORE PREPROCESSING\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "BEFORE PREPROCESSING - TRAIN\n",
            "==================================================\n",
            "Number of examples: 4225\n",
            "Mean sentence length: 11.03 words\n",
            "Mean SQL query length: 64.81 words\n",
            "Vocabulary size (natural language): 860\n",
            "Vocabulary size (SQL): 632\n",
            "\n",
            "==================================================\n",
            "BEFORE PREPROCESSING - DEV\n",
            "==================================================\n",
            "Number of examples: 466\n",
            "Mean sentence length: 10.98 words\n",
            "Mean SQL query length: 62.67 words\n",
            "Vocabulary size (natural language): 442\n",
            "Vocabulary size (SQL): 387\n",
            "\n",
            "============================================================\n",
            "TABLE 2: AFTER PREPROCESSING\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "AFTER PREPROCESSING (T5) - TRAIN\n",
            "==================================================\n",
            "Model name: google-t5/t5-small\n",
            "Mean sentence length: 17.10 tokens\n",
            "Mean SQL query length: 216.37 tokens\n",
            "Vocabulary size (natural language): 791 unique token IDs\n",
            "Vocabulary size (SQL): 555 unique token IDs\n",
            "\n",
            "==================================================\n",
            "AFTER PREPROCESSING (T5) - DEV\n",
            "==================================================\n",
            "Model name: google-t5/t5-small\n",
            "Mean sentence length: 17.07 tokens\n",
            "Mean SQL query length: 210.05 tokens\n",
            "Vocabulary size (natural language): 465 unique token IDs\n",
            "Vocabulary size (SQL): 395 unique token IDs\n",
            "\n",
            "============================================================\n",
            "FORMATTED FOR REPORT\n",
            "============================================================\n",
            "\n",
            "Table 1: Data statistics before any pre-processing\n",
            "----------------------------------------------------------------------\n",
            "Statistics Name                          Train           Dev            \n",
            "----------------------------------------------------------------------\n",
            "Number of examples                       4225            466            \n",
            "Mean sentence length (words)             11.03           10.98          \n",
            "Mean SQL query length (words)            64.81           62.67          \n",
            "Vocabulary size (natural language)       860             442            \n",
            "Vocabulary size (SQL)                    632             387            \n",
            "\n",
            "\n",
            "Table 2: Data statistics after pre-processing\n",
            "----------------------------------------------------------------------\n",
            "Model name: google-t5/t5-small          \n",
            "----------------------------------------------------------------------\n",
            "Statistics Name                          Train           Dev            \n",
            "----------------------------------------------------------------------\n",
            "Mean sentence length (tokens)            17.10           17.07          \n",
            "Mean SQL query length (tokens)           216.37          210.05         \n",
            "Vocabulary size (natural language)       791             465            \n",
            "Vocabulary size (SQL)                    555             395            \n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Calculate Q4 Statistics\n",
        "import os\n",
        "from transformers import T5TokenizerFast\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Initialize T5 tokenizer\n",
        "tokenizer = T5TokenizerFast.from_pretrained('google-t5/t5-small')\n",
        "\n",
        "def load_lines(file_path):\n",
        "    \"\"\"Load lines from a file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [line.strip() for line in f.readlines()]\n",
        "    return lines\n",
        "\n",
        "def calculate_stats_before_preprocessing(nl_path, sql_path, split_name):\n",
        "    \"\"\"Calculate statistics before preprocessing (raw text)\"\"\"\n",
        "\n",
        "    # Load data\n",
        "    nl_queries = load_lines(nl_path)\n",
        "    sql_queries = load_lines(sql_path)\n",
        "\n",
        "    # Number of examples\n",
        "    num_examples = len(nl_queries)\n",
        "\n",
        "    # Tokenize with NLTK (word-level for raw stats)\n",
        "    nl_words = [nltk.word_tokenize(q.lower()) for q in nl_queries]\n",
        "    sql_words = [nltk.word_tokenize(q.lower()) for q in sql_queries]\n",
        "\n",
        "    # Mean lengths (in words)\n",
        "    mean_nl_length = np.mean([len(words) for words in nl_words])\n",
        "    mean_sql_length = np.mean([len(words) for words in sql_words])\n",
        "\n",
        "    # Vocabulary sizes\n",
        "    nl_vocab = set()\n",
        "    for words in nl_words:\n",
        "        nl_vocab.update(words)\n",
        "\n",
        "    sql_vocab = set()\n",
        "    for words in sql_words:\n",
        "        sql_vocab.update(words)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"BEFORE PREPROCESSING - {split_name.upper()}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Number of examples: {num_examples}\")\n",
        "    print(f\"Mean sentence length: {mean_nl_length:.2f} words\")\n",
        "    print(f\"Mean SQL query length: {mean_sql_length:.2f} words\")\n",
        "    print(f\"Vocabulary size (natural language): {len(nl_vocab)}\")\n",
        "    print(f\"Vocabulary size (SQL): {len(sql_vocab)}\")\n",
        "\n",
        "    return {\n",
        "        'num_examples': num_examples,\n",
        "        'mean_nl_length': mean_nl_length,\n",
        "        'mean_sql_length': mean_sql_length,\n",
        "        'nl_vocab_size': len(nl_vocab),\n",
        "        'sql_vocab_size': len(sql_vocab)\n",
        "    }\n",
        "\n",
        "def calculate_stats_after_preprocessing(nl_path, sql_path, split_name):\n",
        "    \"\"\"Calculate statistics after T5 tokenization\"\"\"\n",
        "\n",
        "    # Load data\n",
        "    nl_queries = load_lines(nl_path)\n",
        "    sql_queries = load_lines(sql_path)\n",
        "\n",
        "    # Tokenize with T5\n",
        "    nl_tokenized = [tokenizer.encode(q, add_special_tokens=False) for q in nl_queries]\n",
        "    sql_tokenized = [tokenizer.encode(q, add_special_tokens=False) for q in sql_queries]\n",
        "\n",
        "    # Mean lengths (in tokens)\n",
        "    mean_nl_length = np.mean([len(tokens) for tokens in nl_tokenized])\n",
        "    mean_sql_length = np.mean([len(tokens) for tokens in sql_tokenized])\n",
        "\n",
        "    # Vocabulary sizes (unique token IDs used)\n",
        "    nl_vocab = set()\n",
        "    for tokens in nl_tokenized:\n",
        "        nl_vocab.update(tokens)\n",
        "\n",
        "    sql_vocab = set()\n",
        "    for tokens in sql_tokenized:\n",
        "        sql_vocab.update(tokens)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"AFTER PREPROCESSING (T5) - {split_name.upper()}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Model name: google-t5/t5-small\")\n",
        "    print(f\"Mean sentence length: {mean_nl_length:.2f} tokens\")\n",
        "    print(f\"Mean SQL query length: {mean_sql_length:.2f} tokens\")\n",
        "    print(f\"Vocabulary size (natural language): {len(nl_vocab)} unique token IDs\")\n",
        "    print(f\"Vocabulary size (SQL): {len(sql_vocab)} unique token IDs\")\n",
        "\n",
        "    return {\n",
        "        'mean_nl_length': mean_nl_length,\n",
        "        'mean_sql_length': mean_sql_length,\n",
        "        'nl_vocab_size': len(nl_vocab),\n",
        "        'sql_vocab_size': len(sql_vocab)\n",
        "    }\n",
        "\n",
        "# Change to part-2-code directory\n",
        "data_dir = '/content/hw4/part-2-code/data'\n",
        "\n",
        "# Calculate statistics for TRAIN set\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TABLE 1: BEFORE PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_before = calculate_stats_before_preprocessing(\n",
        "    f'{data_dir}/train.nl',\n",
        "    f'{data_dir}/train.sql',\n",
        "    'train'\n",
        ")\n",
        "\n",
        "dev_before = calculate_stats_before_preprocessing(\n",
        "    f'{data_dir}/dev.nl',\n",
        "    f'{data_dir}/dev.sql',\n",
        "    'dev'\n",
        ")\n",
        "\n",
        "# Calculate statistics for AFTER preprocessing\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TABLE 2: AFTER PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_after = calculate_stats_after_preprocessing(\n",
        "    f'{data_dir}/train.nl',\n",
        "    f'{data_dir}/train.sql',\n",
        "    'train'\n",
        ")\n",
        "\n",
        "dev_after = calculate_stats_after_preprocessing(\n",
        "    f'{data_dir}/dev.nl',\n",
        "    f'{data_dir}/dev.sql',\n",
        "    'dev'\n",
        ")\n",
        "\n",
        "# Print formatted tables for report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FORMATTED FOR REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nTable 1: Data statistics before any pre-processing\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Statistics Name':<40} {'Train':<15} {'Dev':<15}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Number of examples':<40} {train_before['num_examples']:<15} {dev_before['num_examples']:<15}\")\n",
        "print(f\"{'Mean sentence length (words)':<40} {train_before['mean_nl_length']:<15.2f} {dev_before['mean_nl_length']:<15.2f}\")\n",
        "print(f\"{'Mean SQL query length (words)':<40} {train_before['mean_sql_length']:<15.2f} {dev_before['mean_sql_length']:<15.2f}\")\n",
        "print(f\"{'Vocabulary size (natural language)':<40} {train_before['nl_vocab_size']:<15} {dev_before['nl_vocab_size']:<15}\")\n",
        "print(f\"{'Vocabulary size (SQL)':<40} {train_before['sql_vocab_size']:<15} {dev_before['sql_vocab_size']:<15}\")\n",
        "\n",
        "print(\"\\n\\nTable 2: Data statistics after pre-processing\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Model name: google-t5/t5-small':<40}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Statistics Name':<40} {'Train':<15} {'Dev':<15}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Mean sentence length (tokens)':<40} {train_after['mean_nl_length']:<15.2f} {dev_after['mean_nl_length']:<15.2f}\")\n",
        "print(f\"{'Mean SQL query length (tokens)':<40} {train_after['mean_sql_length']:<15.2f} {dev_after['mean_sql_length']:<15.2f}\")\n",
        "print(f\"{'Vocabulary size (natural language)':<40} {train_after['nl_vocab_size']:<15} {dev_after['nl_vocab_size']:<15}\")\n",
        "print(f\"{'Vocabulary size (SQL)':<40} {train_after['sql_vocab_size']:<15} {dev_after['sql_vocab_size']:<15}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T6sP1BcYfyU",
        "outputId": "85410c9c-8d11-406a-e02a-c7b9d8c72504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/hw4/part-2-code/load_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/hw4/part-2-code/load_data.py\n",
        "import os, random, re, string\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from transformers import T5TokenizerFast\n",
        "import torch\n",
        "\n",
        "PAD_IDX = 0\n",
        "\n",
        "class T5Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_folder, split):\n",
        "        '''\n",
        "        Dataset class for T5 model.\n",
        "\n",
        "        Args:\n",
        "            data_folder: Path to data directory\n",
        "            split: 'train', 'dev', or 'test'\n",
        "        '''\n",
        "        self.split = split\n",
        "        self.tokenizer = T5TokenizerFast.from_pretrained('google-t5/t5-small')\n",
        "\n",
        "        # Process data\n",
        "        self.data = self.process_data(data_folder, split, self.tokenizer)\n",
        "\n",
        "    def process_data(self, data_folder, split, tokenizer):\n",
        "        '''\n",
        "        Load and tokenize data from .nl and .sql files.\n",
        "        '''\n",
        "        # Load natural language queries\n",
        "        nl_path = os.path.join(data_folder, f'{split}.nl')\n",
        "        with open(nl_path, 'r') as f:\n",
        "            nl_queries = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        # Load SQL queries (except for test set)\n",
        "        if split != 'test':\n",
        "            sql_path = os.path.join(data_folder, f'{split}.sql')\n",
        "            with open(sql_path, 'r') as f:\n",
        "                sql_queries = [line.strip() for line in f.readlines()]\n",
        "        else:\n",
        "            sql_queries = None\n",
        "\n",
        "        # Tokenize\n",
        "        data = []\n",
        "        for i, nl_query in enumerate(nl_queries):\n",
        "            # Tokenize encoder input (natural language)\n",
        "            encoder_input = tokenizer.encode(nl_query, add_special_tokens=True)\n",
        "\n",
        "            if split != 'test':\n",
        "                # Tokenize decoder input and target (SQL)\n",
        "                sql_query = sql_queries[i]\n",
        "\n",
        "                # Decoder input: add BOS token at the beginning\n",
        "                # We'll use pad_token_id as BOS (T5 convention)\n",
        "                decoder_input = tokenizer.encode(sql_query, add_special_tokens=False)\n",
        "                decoder_input = [tokenizer.pad_token_id] + decoder_input\n",
        "\n",
        "                # Decoder target: SQL tokens (shifted by 1 from decoder_input)\n",
        "                decoder_target = tokenizer.encode(sql_query, add_special_tokens=True)\n",
        "\n",
        "                data.append({\n",
        "                    'encoder_input': torch.tensor(encoder_input, dtype=torch.long),\n",
        "                    'decoder_input': torch.tensor(decoder_input, dtype=torch.long),\n",
        "                    'decoder_target': torch.tensor(decoder_target, dtype=torch.long)\n",
        "                })\n",
        "            else:\n",
        "                # Test set: only encoder input\n",
        "                data.append({\n",
        "                    'encoder_input': torch.tensor(encoder_input, dtype=torch.long)\n",
        "                })\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def normal_collate_fn(batch):\n",
        "    '''\n",
        "    Collation function to perform dynamic padding for training and evaluation with the\n",
        "    development or validation set.\n",
        "\n",
        "    Inputs:\n",
        "        * batch (List[Any]): batch is a list of length batch_size, where each index contains what\n",
        "                             the dataset __getitem__ function returns.\n",
        "\n",
        "    Returns: To be compatible with the provided training loop, you should be returning\n",
        "        * encoder_ids: The input ids of shape BxT to be fed into the T5 encoder.\n",
        "        * encoder_mask: Mask of shape BxT associated with padding tokens in the encoder input\n",
        "        * decoder_inputs: Decoder input ids of shape BxT' to be fed into T5 decoder.\n",
        "        * decoder_targets: The target tokens with which to train the decoder (the tokens following each decoder input)\n",
        "        * initial_decoder_inputs: The very first input token to be decoder (only to be used in evaluation)\n",
        "    '''\n",
        "    # Extract from batch\n",
        "    encoder_inputs = [item['encoder_input'] for item in batch]\n",
        "    decoder_inputs = [item['decoder_input'] for item in batch]\n",
        "    decoder_targets = [item['decoder_target'] for item in batch]\n",
        "\n",
        "    # Pad sequences\n",
        "    encoder_ids = pad_sequence(encoder_inputs, batch_first=True, padding_value=PAD_IDX)\n",
        "    decoder_input_ids = pad_sequence(decoder_inputs, batch_first=True, padding_value=PAD_IDX)\n",
        "    decoder_target_ids = pad_sequence(decoder_targets, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # Create attention mask for encoder (1 for real tokens, 0 for padding)\n",
        "    encoder_mask = (encoder_ids != PAD_IDX).long()\n",
        "\n",
        "    # Initial decoder input (just the BOS token)\n",
        "    initial_decoder_inputs = decoder_input_ids[:, 0:1]  # Shape: (B, 1)\n",
        "\n",
        "    return encoder_ids, encoder_mask, decoder_input_ids, decoder_target_ids, initial_decoder_inputs\n",
        "\n",
        "def test_collate_fn(batch):\n",
        "    '''\n",
        "    Collation function to perform dynamic padding for inference on the test set.\n",
        "\n",
        "    Inputs:\n",
        "        * batch (List[Any]): batch is a list of length batch_size, where each index contains what\n",
        "                             the dataset __getitem__ function returns.\n",
        "\n",
        "    Recommended returns:\n",
        "        * encoder_ids: The input ids of shape BxT to be fed into the T5 encoder.\n",
        "        * encoder_mask: Mask of shape BxT associated with padding tokens in the encoder input\n",
        "        * initial_decoder_inputs: The very first input token to be decoder (only to be used in evaluation)\n",
        "    '''\n",
        "    # Extract encoder inputs\n",
        "    encoder_inputs = [item['encoder_input'] for item in batch]\n",
        "\n",
        "    # Pad sequences\n",
        "    encoder_ids = pad_sequence(encoder_inputs, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # Create attention mask\n",
        "    encoder_mask = (encoder_ids != PAD_IDX).long()\n",
        "\n",
        "    # Initial decoder input (BOS token = pad_token_id for T5)\n",
        "    batch_size = encoder_ids.size(0)\n",
        "    initial_decoder_inputs = torch.full((batch_size, 1), PAD_IDX, dtype=torch.long)\n",
        "\n",
        "    return encoder_ids, encoder_mask, initial_decoder_inputs\n",
        "\n",
        "def get_dataloader(batch_size, split):\n",
        "    data_folder = 'data'\n",
        "    dset = T5Dataset(data_folder, split)\n",
        "    shuffle = split == \"train\"\n",
        "    collate_fn = normal_collate_fn if split != \"test\" else test_collate_fn\n",
        "\n",
        "    dataloader = DataLoader(dset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "    return dataloader\n",
        "\n",
        "def load_t5_data(batch_size, test_batch_size):\n",
        "    train_loader = get_dataloader(batch_size, \"train\")\n",
        "    dev_loader = get_dataloader(test_batch_size, \"dev\")\n",
        "    test_loader = get_dataloader(test_batch_size, \"test\")\n",
        "\n",
        "    return train_loader, dev_loader, test_loader\n",
        "\n",
        "\n",
        "def load_lines(path):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [line.strip() for line in lines]\n",
        "    return lines\n",
        "\n",
        "def load_prompting_data(data_folder):\n",
        "    # For prompting approaches (not needed for this assignment)\n",
        "    train_x = load_lines(os.path.join(data_folder, 'train.nl'))\n",
        "    train_y = load_lines(os.path.join(data_folder, 'train.sql'))\n",
        "    dev_x = load_lines(os.path.join(data_folder, 'dev.nl'))\n",
        "    dev_y = load_lines(os.path.join(data_folder, 'dev.sql'))\n",
        "    test_x = load_lines(os.path.join(data_folder, 'test.nl'))\n",
        "\n",
        "    return train_x, train_y, dev_x, dev_y, test_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1gFowR9a1Nw",
        "outputId": "3701d9c1-566e-48a0-a5ba-460af2db4b87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/hw4/part-2-code/t5_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/hw4/part-2-code/t5_utils.py\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import T5ForConditionalGeneration, T5Config\n",
        "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
        "import wandb\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def setup_wandb(args):\n",
        "    # Optional: Implement wandb integration for experiment tracking\n",
        "    if args.use_wandb:\n",
        "        wandb.init(project=\"hw4-t5-text2sql\", name=args.experiment_name, config=vars(args))\n",
        "\n",
        "def initialize_model(args):\n",
        "    '''\n",
        "    Helper function to initialize the model. You should be either finetuning\n",
        "    the pretrained model associated with the 'google-t5/t5-small' checkpoint\n",
        "    or training a T5 model initialized with the 'google-t5/t5-small' config\n",
        "    from scratch.\n",
        "    '''\n",
        "    if args.finetune:\n",
        "        # Fine-tuning: Load pretrained model\n",
        "        print(\"Loading pretrained T5-small model...\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-small')\n",
        "    else:\n",
        "        # Training from scratch: Load config only\n",
        "        print(\"Initializing T5-small model from scratch...\")\n",
        "        config = T5Config.from_pretrained('google-t5/t5-small')\n",
        "        model = T5ForConditionalGeneration(config)\n",
        "\n",
        "    model.to(DEVICE)\n",
        "    print(f\"Model loaded on {DEVICE}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def mkdir(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        try:\n",
        "            os.makedirs(dirpath)\n",
        "        except FileExistsError:\n",
        "            pass\n",
        "\n",
        "def save_model(checkpoint_dir, model, best):\n",
        "    '''\n",
        "    Save model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_dir: Directory to save checkpoint\n",
        "        model: Model to save\n",
        "        best: If True, save as 'best_model', else 'latest_model'\n",
        "    '''\n",
        "    mkdir(checkpoint_dir)\n",
        "\n",
        "    if best:\n",
        "        save_path = os.path.join(checkpoint_dir, 'best_model')\n",
        "    else:\n",
        "        save_path = os.path.join(checkpoint_dir, 'latest_model')\n",
        "\n",
        "    mkdir(save_path)\n",
        "    model.save_pretrained(save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "def load_model_from_checkpoint(args, best):\n",
        "    '''\n",
        "    Load model from checkpoint.\n",
        "\n",
        "    Args:\n",
        "        args: Arguments containing checkpoint_dir\n",
        "        best: If True, load 'best_model', else 'latest_model'\n",
        "    '''\n",
        "    if best:\n",
        "        load_path = os.path.join(args.checkpoint_dir, 'best_model')\n",
        "    else:\n",
        "        load_path = os.path.join(args.checkpoint_dir, 'latest_model')\n",
        "\n",
        "    print(f\"Loading model from {load_path}...\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(load_path)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    return model\n",
        "\n",
        "def initialize_optimizer_and_scheduler(args, model, epoch_length):\n",
        "    optimizer = initialize_optimizer(args, model)\n",
        "    scheduler = initialize_scheduler(args, optimizer, epoch_length)\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def initialize_optimizer(args, model):\n",
        "    decay_parameters = get_parameter_names(model, transformers.pytorch_utils.ALL_LAYERNORM_LAYERS)\n",
        "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in model.named_parameters() if (n in decay_parameters and p.requires_grad)\n",
        "            ],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p for n, p in model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if args.optimizer_type == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            optimizer_grouped_parameters, lr=args.learning_rate, eps=1e-8, betas=(0.9, 0.999)\n",
        "        )\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def initialize_scheduler(args, optimizer, epoch_length):\n",
        "    num_training_steps = epoch_length * args.max_n_epochs\n",
        "    num_warmup_steps = epoch_length * args.num_warmup_epochs\n",
        "\n",
        "    if args.scheduler_type == \"none\":\n",
        "        return None\n",
        "    elif args.scheduler_type == \"cosine\":\n",
        "        return transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "    elif args.scheduler_type == \"linear\":\n",
        "        return transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "def get_parameter_names(model, forbidden_layer_types):\n",
        "    result = []\n",
        "    for name, child in model.named_children():\n",
        "        result += [\n",
        "            f\"{name}.{n}\"\n",
        "            for n in get_parameter_names(child, forbidden_layer_types)\n",
        "            if not isinstance(child, tuple(forbidden_layer_types))\n",
        "        ]\n",
        "    # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n",
        "    result += list(model._parameters.keys())\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l45iv-Ha9MX",
        "outputId": "0e7c6a8b-55e7-4c3b-8f4f-269d33bf1f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/hw4/part-2-code/train_t5.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/hw4/part-2-code/train_t5.py\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "from t5_utils import initialize_model, initialize_optimizer_and_scheduler, save_model, load_model_from_checkpoint, setup_wandb\n",
        "from transformers import GenerationConfig\n",
        "from load_data import load_t5_data\n",
        "from utils import compute_metrics, save_queries_and_records\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "PAD_IDX = 0\n",
        "\n",
        "def get_args():\n",
        "    '''\n",
        "    Arguments for training. You may choose to change or extend these as you see fit.\n",
        "    '''\n",
        "    parser = argparse.ArgumentParser(description='T5 training loop')\n",
        "\n",
        "    # Model hyperparameters\n",
        "    parser.add_argument('--finetune', action='store_true', help=\"Whether to finetune T5 or not\")\n",
        "\n",
        "    # Training hyperparameters\n",
        "    parser.add_argument('--optimizer_type', type=str, default=\"AdamW\", choices=[\"AdamW\"],\n",
        "                        help=\"What optimizer to use\")\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-1)\n",
        "    parser.add_argument('--weight_decay', type=float, default=0)\n",
        "\n",
        "    parser.add_argument('--scheduler_type', type=str, default=\"cosine\", choices=[\"none\", \"cosine\", \"linear\"],\n",
        "                        help=\"Whether to use a LR scheduler and what type to use if so\")\n",
        "    parser.add_argument('--num_warmup_epochs', type=int, default=0,\n",
        "                        help=\"How many epochs to warm up the learning rate for if using a scheduler\")\n",
        "    parser.add_argument('--max_n_epochs', type=int, default=0,\n",
        "                        help=\"How many epochs to train the model for\")\n",
        "    parser.add_argument('--patience_epochs', type=int, default=0,\n",
        "                        help=\"If validation performance stops improving, how many epochs should we wait before stopping?\")\n",
        "\n",
        "    parser.add_argument('--use_wandb', action='store_true',\n",
        "                        help=\"If set, we will use wandb to keep track of experiments\")\n",
        "    parser.add_argument('--experiment_name', type=str, default='experiment',\n",
        "                        help=\"How should we name this experiment?\")\n",
        "\n",
        "    # Data hyperparameters\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "    parser.add_argument('--test_batch_size', type=int, default=16)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def train(args, model, train_loader, dev_loader, optimizer, scheduler):\n",
        "    best_f1 = -1\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "    model_type = 'ft' if args.finetune else 'scr'\n",
        "    checkpoint_dir = os.path.join('checkpoints', f'{model_type}_experiments', args.experiment_name)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    args.checkpoint_dir = checkpoint_dir\n",
        "    experiment_name = args.experiment_name\n",
        "    gt_sql_path = os.path.join(f'data/dev.sql')\n",
        "    gt_record_path = os.path.join(f'records/ground_truth_dev.pkl')\n",
        "    model_sql_path = os.path.join(f'results/t5_{model_type}_{experiment_name}_dev.sql')\n",
        "    model_record_path = os.path.join(f'records/t5_{model_type}_{experiment_name}_dev.pkl')\n",
        "    for epoch in range(args.max_n_epochs):\n",
        "        tr_loss = train_epoch(args, model, train_loader, optimizer, scheduler)\n",
        "        print(f\"Epoch {epoch}: Average train loss was {tr_loss}\")\n",
        "\n",
        "        eval_loss, record_f1, record_em, sql_em, error_rate = eval_epoch(args, model, dev_loader,\n",
        "                                                                         gt_sql_path, model_sql_path,\n",
        "                                                                         gt_record_path, model_record_path)\n",
        "        print(f\"Epoch {epoch}: Dev loss: {eval_loss}, Record F1: {record_f1}, Record EM: {record_em}, SQL EM: {sql_em}\")\n",
        "        print(f\"Epoch {epoch}: {error_rate*100:.2f}% of the generated outputs led to SQL errors\")\n",
        "\n",
        "        if args.use_wandb:\n",
        "            result_dict = {\n",
        "                'train/loss' : tr_loss,\n",
        "                'dev/loss' : eval_loss,\n",
        "                'dev/record_f1' : record_f1,\n",
        "                'dev/record_em' : record_em,\n",
        "                'dev/sql_em' : sql_em,\n",
        "                'dev/error_rate' : error_rate,\n",
        "            }\n",
        "            wandb.log(result_dict, step=epoch)\n",
        "\n",
        "        if record_f1 > best_f1:\n",
        "            best_f1 = record_f1\n",
        "            epochs_since_improvement = 0\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "\n",
        "        save_model(checkpoint_dir, model, best=False)\n",
        "        if epochs_since_improvement == 0:\n",
        "            save_model(checkpoint_dir, model, best=True)\n",
        "\n",
        "        if epochs_since_improvement >= args.patience_epochs:\n",
        "            break\n",
        "\n",
        "def train_epoch(args, model, train_loader, optimizer, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for encoder_input, encoder_mask, decoder_input, decoder_targets, _ in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        encoder_input = encoder_input.to(DEVICE)\n",
        "        encoder_mask = encoder_mask.to(DEVICE)\n",
        "        decoder_input = decoder_input.to(DEVICE)\n",
        "        decoder_targets = decoder_targets.to(DEVICE)\n",
        "\n",
        "        logits = model(\n",
        "            input_ids=encoder_input,\n",
        "            attention_mask=encoder_mask,\n",
        "            decoder_input_ids=decoder_input,\n",
        "        )['logits']\n",
        "\n",
        "        non_pad = decoder_targets != PAD_IDX\n",
        "        loss = criterion(logits[non_pad], decoder_targets[non_pad])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            num_tokens = torch.sum(non_pad).item()\n",
        "            total_loss += loss.item() * num_tokens\n",
        "            total_tokens += num_tokens\n",
        "\n",
        "    return total_loss / total_tokens\n",
        "\n",
        "def eval_epoch(args, model, dev_loader, gt_sql_path, model_sql_path, gt_record_path, model_record_path):\n",
        "    '''\n",
        "    Evaluation loop for dev set during training.\n",
        "\n",
        "    Returns:\n",
        "        eval_loss: Cross-entropy loss\n",
        "        record_f1: F1 score on database records\n",
        "        record_em: Exact match on database records\n",
        "        sql_em: Exact match on SQL queries\n",
        "        error_rate: Percentage of queries with SQL errors\n",
        "    '''\n",
        "    model.eval()\n",
        "    from transformers import T5TokenizerFast\n",
        "    tokenizer = T5TokenizerFast.from_pretrained('google-t5/t5-small')\n",
        "\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    all_generated_queries = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoder_input, encoder_mask, decoder_input, decoder_targets, initial_decoder_input in tqdm(dev_loader):\n",
        "            encoder_input = encoder_input.to(DEVICE)\n",
        "            encoder_mask = encoder_mask.to(DEVICE)\n",
        "            decoder_input = decoder_input.to(DEVICE)\n",
        "            decoder_targets = decoder_targets.to(DEVICE)\n",
        "            initial_decoder_input = initial_decoder_input.to(DEVICE)\n",
        "\n",
        "            # Compute loss\n",
        "            logits = model(\n",
        "                input_ids=encoder_input,\n",
        "                attention_mask=encoder_mask,\n",
        "                decoder_input_ids=decoder_input,\n",
        "            )['logits']\n",
        "\n",
        "            non_pad = decoder_targets != PAD_IDX\n",
        "            loss = criterion(logits[non_pad], decoder_targets[non_pad])\n",
        "\n",
        "            num_tokens = torch.sum(non_pad).item()\n",
        "            total_loss += loss.item() * num_tokens\n",
        "            total_tokens += num_tokens\n",
        "\n",
        "            # Generate SQL queries\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=encoder_input,\n",
        "                attention_mask=encoder_mask,\n",
        "                max_length=512,\n",
        "                num_beams=5,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            # Decode generated IDs to SQL strings\n",
        "            generated_queries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            all_generated_queries.extend(generated_queries)\n",
        "\n",
        "    # Save queries and compute metrics\n",
        "    save_queries_and_records(all_generated_queries, model_sql_path, model_record_path)\n",
        "\n",
        "    sql_em, record_em, record_f1, error_msgs = compute_metrics(\n",
        "        gt_sql_path, model_sql_path, gt_record_path, model_record_path\n",
        "    )\n",
        "\n",
        "    # Calculate error rate\n",
        "    num_errors = sum([1 for msg in error_msgs if msg != \"\"])\n",
        "    error_rate = num_errors / len(error_msgs) if len(error_msgs) > 0 else 0\n",
        "\n",
        "    eval_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "    return eval_loss, record_f1, record_em, sql_em, error_rate\n",
        "\n",
        "def test_inference(args, model, test_loader, model_sql_path, model_record_path):\n",
        "    '''\n",
        "    Generate predictions for test set.\n",
        "    '''\n",
        "    model.eval()\n",
        "    from transformers import T5TokenizerFast\n",
        "    tokenizer = T5TokenizerFast.from_pretrained('google-t5/t5-small')\n",
        "\n",
        "    all_generated_queries = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoder_input, encoder_mask, initial_decoder_input in tqdm(test_loader):\n",
        "            encoder_input = encoder_input.to(DEVICE)\n",
        "            encoder_mask = encoder_mask.to(DEVICE)\n",
        "\n",
        "            # Generate SQL queries\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=encoder_input,\n",
        "                attention_mask=encoder_mask,\n",
        "                max_length=512,\n",
        "                num_beams=5,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            # Decode to SQL strings\n",
        "            generated_queries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            all_generated_queries.extend(generated_queries)\n",
        "\n",
        "    # Save queries and records\n",
        "    save_queries_and_records(all_generated_queries, model_sql_path, model_record_path)\n",
        "    print(f\"Test predictions saved to {model_sql_path} and {model_record_path}\")\n",
        "\n",
        "def main():\n",
        "    # Get key arguments\n",
        "    args = get_args()\n",
        "    if args.use_wandb:\n",
        "        # Recommended: Using wandb (or tensorboard) for result logging can make experimentation easier\n",
        "        setup_wandb(args)\n",
        "\n",
        "    # Load the data and the model\n",
        "    train_loader, dev_loader, test_loader = load_t5_data(args.batch_size, args.test_batch_size)\n",
        "    model = initialize_model(args)\n",
        "    optimizer, scheduler = initialize_optimizer_and_scheduler(args, model, len(train_loader))\n",
        "\n",
        "    # Train\n",
        "    train(args, model, train_loader, dev_loader, optimizer, scheduler)\n",
        "\n",
        "    # Evaluate\n",
        "    model = load_model_from_checkpoint(args, best=True)\n",
        "    model.eval()\n",
        "\n",
        "    # Dev set\n",
        "    experiment_name = args.experiment_name\n",
        "    model_type = 'ft' if args.finetune else 'scr'\n",
        "    gt_sql_path = os.path.join(f'data/dev.sql')\n",
        "    gt_record_path = os.path.join(f'records/ground_truth_dev.pkl')\n",
        "    model_sql_path = os.path.join(f'results/t5_{model_type}_{experiment_name}_dev.sql')\n",
        "    model_record_path = os.path.join(f'records/t5_{model_type}_{experiment_name}_dev.pkl')\n",
        "    dev_loss, dev_record_em, dev_record_f1, dev_sql_em, dev_error_rate = eval_epoch(args, model, dev_loader,\n",
        "                                                                                    gt_sql_path, model_sql_path,\n",
        "                                                                                    gt_record_path, model_record_path)\n",
        "    print(\"Dev set results: Loss: {dev_loss}, Record F1: {dev_record_f1}, Record EM: {dev_record_em}, SQL EM: {dev_sql_em}\")\n",
        "    print(f\"Dev set results: {dev_error_rate*100:.2f}% of the generated outputs led to SQL errors\")\n",
        "\n",
        "    # Test set\n",
        "    model_sql_path = os.path.join(f'results/t5_{model_type}_{experiment_name}_test.sql')\n",
        "    model_record_path = os.path.join(f'records/t5_{model_type}_{experiment_name}_test.pkl')\n",
        "    test_inference(args, model, test_loader, model_sql_path, model_record_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX-en4ybbF-d",
        "outputId": "68181b90-36a8-44b0-fdbe-983c8e0aacca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/hw4/part-2-code\n",
            "2025-11-15 22:17:47.555759: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-15 22:17:47.573681: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763245067.595419   54758 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763245067.601938   54758 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763245067.618626   54758 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763245067.618654   54758 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763245067.618657   54758 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763245067.618660   54758 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-15 22:17:47.623634: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Loading pretrained T5-small model...\n",
            "Model loaded on cuda\n",
            "100% 529/529 [00:45<00:00, 11.72it/s]\n",
            "Epoch 0: Average train loss was 3.149552680488379\n",
            "100% 30/30 [03:05<00:00,  6.20s/it]\n",
            "466it [00:00, 1119.64it/s]\n",
            "Epoch 0: Dev loss: 0.5705393930946111, Record F1: 0.11804672041174558, Record EM: 0.11802575107296137, SQL EM: 0.0\n",
            "Epoch 0: 98.71% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [00:44<00:00, 11.85it/s]\n",
            "Epoch 1: Average train loss was 0.5211924504745595\n",
            "100% 30/30 [02:33<00:00,  5.12s/it]\n",
            "466it [00:07, 59.17it/s]\n",
            "Epoch 1: Dev loss: 0.23566657332793145, Record F1: 0.2929253825785744, Record EM: 0.19313304721030042, SQL EM: 0.0\n",
            "Epoch 1: 20.39% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [00:44<00:00, 11.84it/s]\n",
            "Epoch 2: Average train loss was 0.28591451190465744\n",
            "100% 30/30 [02:50<00:00,  5.70s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 2: Dev loss: 0.15459359422288363, Record F1: 0.36711454101468854, Record EM: 0.23390557939914164, SQL EM: 0.002145922746781116\n",
            "Epoch 2: 14.59% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [00:44<00:00, 11.84it/s]\n",
            "Epoch 3: Average train loss was 0.21012534091513457\n",
            "100% 30/30 [02:07<00:00,  4.25s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 3: Dev loss: 0.11829306397336804, Record F1: 0.39515259448336043, Record EM: 0.27896995708154504, SQL EM: 0.004291845493562232\n",
            "Epoch 3: 16.95% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [00:44<00:00, 11.83it/s]\n",
            "Epoch 4: Average train loss was 0.17075361845414297\n",
            "100% 30/30 [02:02<00:00,  4.08s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 4: Dev loss: 0.10170062389812437, Record F1: 0.4489396490255141, Record EM: 0.33905579399141633, SQL EM: 0.008583690987124463\n",
            "Epoch 4: 17.81% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [00:45<00:00, 11.69it/s]\n",
            "Epoch 5: Average train loss was 0.14676507241411502\n",
            "100% 30/30 [02:01<00:00,  4.05s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 5: Dev loss: 0.09041765284636478, Record F1: 0.4703262916237955, Record EM: 0.36909871244635195, SQL EM: 0.01072961373390558\n",
            "Epoch 5: 24.68% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [00:48<00:00, 10.81it/s]\n",
            "Epoch 6: Average train loss was 0.13136505171881555\n",
            "100% 30/30 [02:40<00:00,  5.33s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 6: Dev loss: 0.08237962762800448, Record F1: 0.5099985520218283, Record EM: 0.4206008583690987, SQL EM: 0.01072961373390558\n",
            "Epoch 6: 25.54% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [00:49<00:00, 10.79it/s]\n",
            "Epoch 7: Average train loss was 0.12069531296096886\n",
            "100% 30/30 [02:37<00:00,  5.25s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 7: Dev loss: 0.07856219145964378, Record F1: 0.5222683123401608, Record EM: 0.41416309012875535, SQL EM: 0.012875536480686695\n",
            "Epoch 7: 20.82% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [00:48<00:00, 10.84it/s]\n",
            "Epoch 8: Average train loss was 0.11369852406290853\n",
            "100% 30/30 [02:26<00:00,  4.89s/it]\n",
            "461it [02:00,  3.84it/s]\n",
            "Epoch 8: Dev loss: 0.07497179219487962, Record F1: 0.5465714451868294, Record EM: 0.45278969957081544, SQL EM: 0.012875536480686695\n",
            "Epoch 8: 21.67% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "100% 529/529 [01:07<00:00,  7.85it/s]\n",
            "Epoch 9: Average train loss was 0.10956885279302482\n",
            "100% 30/30 [03:19<00:00,  6.66s/it]\n",
            "462it [02:00,  3.85it/s]\n",
            "Epoch 9: Dev loss: 0.07409406884598845, Record F1: 0.5538109914301907, Record EM: 0.44849785407725323, SQL EM: 0.012875536480686695\n",
            "Epoch 9: 19.96% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/latest_model\n",
            "Model saved to checkpoints/ft_experiments/my_experiment/best_model\n",
            "Loading model from checkpoints/ft_experiments/my_experiment/best_model...\n",
            "100% 30/30 [04:19<00:00,  8.64s/it]\n",
            "461it [02:00,  3.84it/s]\n",
            "Dev set results: Loss: {dev_loss}, Record F1: {dev_record_f1}, Record EM: {dev_record_em}, SQL EM: {dev_sql_em}\n",
            "Dev set results: 20.17% of the generated outputs led to SQL errors\n",
            "100% 27/27 [03:40<00:00,  8.17s/it]\n",
            "432it [01:43,  4.16it/s]\n",
            "Test predictions saved to results/t5_ft_my_experiment_test.sql and records/t5_ft_my_experiment_test.pkl\n",
            "\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.12/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1594, in _shutdown\n",
            "    atexit_call()\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 31, in _python_exit\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1149, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1179, in _wait_for_tstate_lock\n",
            "    self._stop()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1104, in _stop\n",
            "    with _shutdown_locks_lock:\n",
            "         ^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-2-code\n",
        "\n",
        "!python train_t5.py \\\n",
        "  --finetune \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --weight_decay 0.01 \\\n",
        "  --scheduler_type linear \\\n",
        "  --num_warmup_epochs 1 \\\n",
        "  --max_n_epochs 10 \\\n",
        "  --patience_epochs 3 \\\n",
        "  --batch_size 8 \\\n",
        "  --test_batch_size 16 \\\n",
        "  --experiment_name my_experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JEpCJe4KbH1q",
        "outputId": "a538b46d-873b-4bd8-8946-95efb0f78f14"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5d956147-dce5-4649-af7b-b4e550313b2f\", \"t5_ft_experiment_test.sql\", 203337)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_895d0cea-6019-4168-a222-56581ba4b677\", \"t5_ft_experiment_test.pkl\", 61815)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "!cp results/t5_ft_my_experiment_test.sql results/t5_ft_experiment_test.sql\n",
        "!cp records/t5_ft_my_experiment_test.pkl records/t5_ft_experiment_test.pkl\n",
        "\n",
        "files.download('results/t5_ft_experiment_test.sql')\n",
        "files.download('records/t5_ft_experiment_test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_IXVqAI2crp",
        "outputId": "4346aaf6-3cea-4a3b-8c71-a2b793921f47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/hw4/part-2-code\n",
            "2025-11-16 00:28:56.598710: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 00:28:56.616486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763252936.638005   87681 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763252936.644534   87681 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763252936.661198   87681 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763252936.661228   87681 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763252936.661230   87681 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763252936.661235   87681 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 00:28:56.666234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Loading pretrained T5-small model...\n",
            "Model loaded on cuda\n",
            "100% 133/133 [00:40<00:00,  3.25it/s]\n",
            "Epoch 0: Average train loss was 6.724797943394596\n",
            "100% 30/30 [00:35<00:00,  1.18s/it]\n",
            "466it [00:00, 9550.72it/s]\n",
            "Epoch 0: Dev loss: 3.8776663505952613, Record F1: 0.11802575048283262, Record EM: 0.11802575107296137, SQL EM: 0.0\n",
            "Epoch 0: 99.79% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/strategy2_modified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/strategy2_modified/best_model\n",
            "100% 133/133 [00:40<00:00,  3.28it/s]\n",
            "Epoch 1: Average train loss was 3.0575448424954663\n",
            "100% 30/30 [01:53<00:00,  3.78s/it]\n",
            "466it [00:00, 9022.72it/s]\n",
            "Epoch 1: Dev loss: 1.3695524441019085, Record F1: 0.11802575048283262, Record EM: 0.11802575107296137, SQL EM: 0.0\n",
            "Epoch 1: 74.89% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/strategy2_modified/latest_model\n",
            "100% 133/133 [00:40<00:00,  3.31it/s]\n",
            "Epoch 2: Average train loss was 1.320063597482185\n",
            "100% 30/30 [03:07<00:00,  6.24s/it]\n",
            "466it [00:00, 1176.60it/s]\n",
            "Epoch 2: Dev loss: 0.6303001419719569, Record F1: 0.11802575048283262, Record EM: 0.11802575107296137, SQL EM: 0.0\n",
            "Epoch 2: 90.13% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/strategy2_modified/latest_model\n",
            "100% 133/133 [00:40<00:00,  3.29it/s]\n",
            "Epoch 3: Average train loss was 0.7653792110589989\n",
            "100% 30/30 [03:07<00:00,  6.26s/it]\n",
            "466it [00:04, 108.08it/s]\n",
            "Epoch 3: Dev loss: 0.4035512789773441, Record F1: 0.2300667603795743, Record EM: 0.17167381974248927, SQL EM: 0.0\n",
            "Epoch 3: 57.94% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/strategy2_modified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/strategy2_modified/best_model\n",
            "100% 133/133 [00:40<00:00,  3.26it/s]\n",
            "Epoch 4: Average train loss was 0.5448341114135045\n",
            " 13% 4/30 [00:25<02:43,  6.28s/it]Exception ignored in: <generator object tqdm.__iter__ at 0x7e9d9bb73760>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1196, in __iter__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1275, in close\n",
            "    self._decr_instances(self)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 707, in _decr_instances\n",
            "    instances = list(filter(\n",
            "                ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/_weakrefset.py\", line 64, in __iter__\n",
            "    with _IterationGuard(self):\n",
            "         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/_weakrefset.py\", line 17, in __init__\n",
            "    def __init__(self, weakcontainer):\n",
            "\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/hw4/part-2-code/train_t5.py\", line 276, in <module>\n",
            "    main()\n",
            "  File \"/content/hw4/part-2-code/train_t5.py\", line 251, in main\n",
            "    train(args, model, train_loader, dev_loader, optimizer, scheduler)\n",
            "  File \"/content/hw4/part-2-code/train_t5.py\", line 71, in train\n",
            "    eval_loss, record_f1, record_em, sql_em, error_rate = eval_epoch(args, model, dev_loader,\n",
            "                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/hw4/part-2-code/train_t5.py\", line 179, in eval_epoch\n",
            "    generated_ids = model.generate(\n",
            "                    ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 2564, in generate\n",
            "    result = decoding_method(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 3265, in _beam_search\n",
            "    model_outputs = self(**model_inputs, return_dict=True)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\", line 1764, in forward\n",
            "    decoder_outputs = self.decoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\", line 1100, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
            "    return super().__call__(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\", line 711, in forward\n",
            "    cross_attention_outputs = self.layer[1](\n",
            "                              ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\", line 639, in forward\n",
            "    normed_hidden_states = self.layer_norm(hidden_states)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\", line 256, in forward\n",
            "    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-2-code\n",
        "\n",
        "!python train_t5.py \\\n",
        "  --finetune \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --weight_decay 0.01 \\\n",
        "  --scheduler_type cosine \\\n",
        "  --num_warmup_epochs 3 \\\n",
        "  --max_n_epochs 30 \\\n",
        "  --patience_epochs 7 \\\n",
        "  --batch_size 32 \\\n",
        "  --test_batch_size 16 \\\n",
        "  --experiment_name strategy2_modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3OPdTqR-xyP",
        "outputId": "094f49d3-2a85-4bda-cd87-d8f1ab3a36d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/hw4/part-2-code\n",
            "2025-11-16 00:44:22.762294: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 00:44:22.780294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763253862.801840   91656 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763253862.808320   91656 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763253862.824960   91656 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763253862.824991   91656 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763253862.824993   91656 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763253862.824996   91656 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 00:44:22.830077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Loading pretrained T5-small model...\n",
            "Model loaded on cuda\n",
            "100% 265/265 [00:41<00:00,  6.37it/s]\n",
            "Epoch 0: Average train loss was 4.756103526526743\n",
            "100% 30/30 [00:59<00:00,  2.00s/it]\n",
            "466it [00:00, 9278.07it/s]\n",
            "Epoch 0: Dev loss: 1.7231453978339373, Record F1: 0.11802575048283262, Record EM: 0.11802575107296137, SQL EM: 0.0\n",
            "Epoch 0: 92.27% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:41<00:00,  6.43it/s]\n",
            "Epoch 1: Average train loss was 1.3135248321896302\n",
            "100% 30/30 [03:07<00:00,  6.26s/it]\n",
            "466it [00:00, 726.75it/s]\n",
            "Epoch 1: Dev loss: 0.4907936319108693, Record F1: 0.12129895205541118, Record EM: 0.11587982832618025, SQL EM: 0.0\n",
            "Epoch 1: 95.92% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:41<00:00,  6.43it/s]\n",
            "Epoch 2: Average train loss was 0.5465863690145675\n",
            "100% 30/30 [02:35<00:00,  5.20s/it]\n",
            "466it [00:07, 66.32it/s]\n",
            "Epoch 2: Dev loss: 0.2746544987538404, Record F1: 0.2630182375868614, Record EM: 0.16952789699570817, SQL EM: 0.0\n",
            "Epoch 2: 27.25% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:41<00:00,  6.42it/s]\n",
            "Epoch 3: Average train loss was 0.3452783094370392\n",
            "100% 30/30 [02:49<00:00,  5.66s/it]\n",
            "466it [00:07, 59.37it/s]\n",
            "Epoch 3: Dev loss: 0.19352705919421442, Record F1: 0.3250632932270476, Record EM: 0.21244635193133046, SQL EM: 0.0\n",
            "Epoch 3: 22.96% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:41<00:00,  6.46it/s]\n",
            "Epoch 4: Average train loss was 0.25872861081741855\n",
            "100% 30/30 [02:31<00:00,  5.05s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 4: Dev loss: 0.14882949860107178, Record F1: 0.36682516460362785, Record EM: 0.24034334763948498, SQL EM: 0.0\n",
            "Epoch 4: 20.82% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:41<00:00,  6.42it/s]\n",
            "Epoch 5: Average train loss was 0.2079529892565104\n",
            "100% 30/30 [02:19<00:00,  4.66s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 5: Dev loss: 0.12334404535958787, Record F1: 0.38459868472598785, Record EM: 0.2703862660944206, SQL EM: 0.004291845493562232\n",
            "Epoch 5: 19.96% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:40<00:00,  6.48it/s]\n",
            "Epoch 6: Average train loss was 0.1764037890550062\n",
            "100% 30/30 [02:10<00:00,  4.36s/it]\n",
            "462it [02:00,  3.85it/s]\n",
            "Epoch 6: Dev loss: 0.1059795886589051, Record F1: 0.4223879450536573, Record EM: 0.315450643776824, SQL EM: 0.006437768240343348\n",
            "Epoch 6: 19.10% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:42<00:00,  6.29it/s]\n",
            "Epoch 7: Average train loss was 0.15293065324268978\n",
            "100% 30/30 [02:50<00:00,  5.68s/it]\n",
            "466it [00:48,  9.63it/s]\n",
            "Epoch 7: Dev loss: 0.09443325328629154, Record F1: 0.46801675127541464, Record EM: 0.3583690987124464, SQL EM: 0.01072961373390558\n",
            "Epoch 7: 26.39% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:42<00:00,  6.29it/s]\n",
            "Epoch 8: Average train loss was 0.136585073641265\n",
            "100% 30/30 [02:32<00:00,  5.08s/it]\n",
            "463it [02:00,  3.86it/s]\n",
            "Epoch 8: Dev loss: 0.08592467811172064, Record F1: 0.4908387621004396, Record EM: 0.3927038626609442, SQL EM: 0.008583690987124463\n",
            "Epoch 8: 24.46% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:42<00:00,  6.30it/s]\n",
            "Epoch 9: Average train loss was 0.12346204407965863\n",
            "100% 30/30 [02:44<00:00,  5.49s/it]\n",
            "463it [02:00,  3.86it/s]\n",
            "Epoch 9: Dev loss: 0.07890882526665013, Record F1: 0.5211100091391084, Record EM: 0.40987124463519314, SQL EM: 0.01072961373390558\n",
            "Epoch 9: 17.17% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:46<00:00,  5.66it/s]\n",
            "Epoch 10: Average train loss was 0.11358501379570397\n",
            "100% 30/30 [02:54<00:00,  5.80s/it]\n",
            "462it [02:00,  3.85it/s]\n",
            "Epoch 10: Dev loss: 0.07375158191679777, Record F1: 0.5409286484233062, Record EM: 0.4334763948497854, SQL EM: 0.015021459227467811\n",
            "Epoch 10: 19.74% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:47<00:00,  5.54it/s]\n",
            "Epoch 11: Average train loss was 0.10536161762153927\n",
            "100% 30/30 [03:43<00:00,  7.47s/it]\n",
            "462it [02:00,  3.85it/s]\n",
            "Epoch 11: Dev loss: 0.0696887252368909, Record F1: 0.5816245117082707, Record EM: 0.47639484978540775, SQL EM: 0.015021459227467811\n",
            "Epoch 11: 17.17% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/best_model\n",
            "100% 265/265 [00:48<00:00,  5.44it/s]\n",
            "Epoch 12: Average train loss was 0.0996192728847998\n",
            "100% 30/30 [04:18<00:00,  8.60s/it]\n",
            "391it [02:00,  3.26it/s]\n",
            "Epoch 12: Dev loss: 0.06649860631436998, Record F1: 0.48260910424106523, Record EM: 0.4034334763948498, SQL EM: 0.012875536480686695\n",
            "Epoch 12: 31.12% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "100% 265/265 [01:01<00:00,  4.34it/s]\n",
            "Epoch 13: Average train loss was 0.09312329082662589\n",
            "100% 30/30 [06:08<00:00, 12.29s/it]\n",
            "356it [02:00,  2.97it/s]\n",
            "Epoch 13: Dev loss: 0.06271859282512876, Record F1: 0.4782022028443181, Record EM: 0.4034334763948498, SQL EM: 0.015021459227467811\n",
            "Epoch 13: 36.27% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "100% 265/265 [01:05<00:00,  4.08it/s]\n",
            "Epoch 14: Average train loss was 0.08823890352816906\n",
            "100% 30/30 [07:25<00:00, 14.85s/it]\n",
            "410it [02:00,  3.42it/s]\n",
            "Epoch 14: Dev loss: 0.06054343401361807, Record F1: 0.568435779456909, Record EM: 0.4871244635193133, SQL EM: 0.012875536480686695\n",
            "Epoch 14: 24.68% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "100% 265/265 [01:19<00:00,  3.31it/s]\n",
            "Epoch 15: Average train loss was 0.08403923114592293\n",
            "100% 30/30 [09:29<00:00, 18.98s/it]\n",
            "413it [02:00,  3.44it/s]\n",
            "Epoch 15: Dev loss: 0.05861899064582073, Record F1: 0.5374413821421461, Record EM: 0.4721030042918455, SQL EM: 0.017167381974248927\n",
            "Epoch 15: 27.47% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "100% 265/265 [01:28<00:00,  3.01it/s]\n",
            "Epoch 16: Average train loss was 0.08127883046160042\n",
            "100% 30/30 [10:21<00:00, 20.73s/it]\n",
            "397it [02:00,  3.31it/s]\n",
            "Epoch 16: Dev loss: 0.05635823277148013, Record F1: 0.5235716118221804, Record EM: 0.4592274678111588, SQL EM: 0.017167381974248927\n",
            "Epoch 16: 30.69% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "100% 265/265 [01:35<00:00,  2.76it/s]\n",
            "Epoch 17: Average train loss was 0.07845246840909725\n",
            "100% 30/30 [10:36<00:00, 21.20s/it]\n",
            "387it [02:00,  3.22it/s]\n",
            "Epoch 17: Dev loss: 0.05546906631503899, Record F1: 0.5242162758013257, Record EM: 0.45064377682403434, SQL EM: 0.012875536480686695\n",
            "Epoch 17: 30.47% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "100% 265/265 [01:35<00:00,  2.77it/s]\n",
            "Epoch 18: Average train loss was 0.07716856208461227\n",
            "100% 30/30 [12:28<00:00, 24.94s/it]\n",
            "387it [02:00,  3.22it/s]\n",
            "Epoch 18: Dev loss: 0.05371781157988766, Record F1: 0.5226669677545165, Record EM: 0.45064377682403434, SQL EM: 0.017167381974248927\n",
            "Epoch 18: 30.26% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "100% 265/265 [01:44<00:00,  2.53it/s]\n",
            "Epoch 19: Average train loss was 0.07500500358856471\n",
            "100% 30/30 [13:33<00:00, 27.10s/it]\n",
            "391it [02:00,  3.26it/s]\n",
            "Epoch 19: Dev loss: 0.05302812389636944, Record F1: 0.5273215559029737, Record EM: 0.4613733905579399, SQL EM: 0.017167381974248927\n",
            "Epoch 19: 29.61% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/a100_no_clipping/latest_model\n",
            "Loading model from checkpoints/ft_experiments/a100_no_clipping/best_model...\n",
            "100% 30/30 [14:52<00:00, 29.75s/it]\n",
            "294it [02:00,  2.45it/s]\n",
            "Dev set results: Loss: {dev_loss}, Record F1: {dev_record_f1}, Record EM: {dev_record_em}, SQL EM: {dev_sql_em}\n",
            "Dev set results: 49.36% of the generated outputs led to SQL errors\n",
            "100% 27/27 [12:27<00:00, 27.68s/it]\n",
            "240it [02:00,  2.00it/s]\n",
            "Test predictions saved to results/t5_ft_a100_no_clipping_test.sql and records/t5_ft_a100_no_clipping_test.pkl\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.12/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1594, in _shutdown\n",
            "    atexit_call()\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 31, in _python_exit\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1149, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-2-code\n",
        "!python train_t5.py \\\n",
        "  --finetune \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --weight_decay 0.01 \\\n",
        "  --scheduler_type cosine \\\n",
        "  --num_warmup_epochs 2 \\\n",
        "  --max_n_epochs 25 \\\n",
        "  --patience_epochs 8 \\\n",
        "  --batch_size 16 \\\n",
        "  --test_batch_size 16 \\\n",
        "  --experiment_name a100_no_clipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zokvv5sLHpyO",
        "outputId": "36a49e47-1715-413a-dcec-5bb0454154a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-16 04:33:54.305957: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 04:33:54.324510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763267634.346094  150037 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763267634.352600  150037 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763267634.369492  150037 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763267634.369522  150037 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763267634.369525  150037 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763267634.369528  150037 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 04:33:54.374665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Loading pretrained T5-small model...\n",
            "Model loaded on cuda\n",
            "100% 265/265 [00:41<00:00,  6.43it/s]\n",
            "Epoch 0: Average train loss was 5.235693655663781\n",
            "100% 30/30 [00:49<00:00,  1.64s/it]\n",
            "466it [00:00, 8646.90it/s]\n",
            "Epoch 0: Dev loss: 2.3255285492881606, Record F1: 0.11802575048283262, Record EM: 0.11802575107296137, SQL EM: 0.0\n",
            "Epoch 0: 97.85% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_65plus/best_model\n",
            "100% 265/265 [00:41<00:00,  6.45it/s]\n",
            "Epoch 1: Average train loss was 1.8018884689542611\n",
            "100% 30/30 [03:09<00:00,  6.30s/it]\n",
            "466it [00:00, 1114.32it/s]\n",
            "Epoch 1: Dev loss: 0.7150507368174418, Record F1: 0.11802575048283262, Record EM: 0.11802575107296137, SQL EM: 0.0\n",
            "Epoch 1: 88.63% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_65plus/latest_model\n",
            "100% 265/265 [00:40<00:00,  6.48it/s]\n",
            "Epoch 2: Average train loss was 0.7451614512206955\n",
            "  0% 0/30 [00:00<?, ?it/s]Exception ignored in: <generator object tqdm.__iter__ at 0x7cd434c6b760>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1196, in __iter__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1275, in close\n",
            "    self._decr_instances(self)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 696, in _decr_instances\n",
            "    with cls._lock:\n",
            "         ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 114, in __exit__\n",
            "    self.release()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 108, in release\n",
            "    lock.release()\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/hw4/part-2-code/train_t5.py\", line 275, in <module>\n",
            "    main()\n",
            "  File \"/content/hw4/part-2-code/train_t5.py\", line 250, in main\n",
            "    train(args, model, train_loader, dev_loader, optimizer, scheduler)\n",
            "  File \"/content/hw4/part-2-code/train_t5.py\", line 71, in train\n",
            "    eval_loss, record_f1, record_em, sql_em, error_rate = eval_epoch(args, model, dev_loader,\n",
            "                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/hw4/part-2-code/train_t5.py\", line 178, in eval_epoch\n",
            "    generated_ids = model.generate(\n",
            "                    ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 2564, in generate\n",
            "    result = decoding_method(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\", line 3377, in _beam_search\n",
            "    model_kwargs[\"past_key_values\"].reorder_cache(beam_idx)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\", line 1308, in reorder_cache\n",
            "    self.self_attention_cache.reorder_cache(beam_idx)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\", line 832, in reorder_cache\n",
            "    self.layers[layer_idx].reorder_cache(beam_idx)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/cache_utils.py\", line 81, in reorder_cache\n",
            "    self.values = self.values.index_select(0, beam_idx.to(self.values.device))\n",
            "                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        " !python train_t5.py \\\n",
        "   --finetune \\\n",
        "   --learning_rate 1e-4 \\\n",
        "   --weight_decay 0.05 \\\n",
        "   --scheduler_type cosine \\\n",
        "   --num_warmup_epochs 3 \\\n",
        "   --max_n_epochs 20 \\\n",
        "   --patience_epochs 12 \\\n",
        "   --batch_size 16 \\\n",
        "   --test_batch_size 16 \\\n",
        "   --experiment_name final_65plus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtLhAagIFVzc",
        "outputId": "c70ad216-84b2-4324-a969-f4c31413e04f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot create directory '/content/drive/MyDrive/hw4/saved_models/epoch11_58.2f1/': No such file or directory\n",
            "cp: cannot create directory '/content/drive/MyDrive/hw4/saved_models/epoch19_52.7f1/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Copy the best model checkpoint to Google Drive\n",
        "!cp -r /content/hw4/part-2-code/checkpoints/ft_experiments/a100_no_clipping/best_model \\\n",
        "  /content/drive/MyDrive/hw4/saved_models/epoch11_58.2f1/\n",
        "\n",
        "# Also save the latest model (epoch 19) just in case\n",
        "!cp -r /content/hw4/part-2-code/checkpoints/ft_experiments/a100_no_clipping/latest_model \\\n",
        "  /content/drive/MyDrive/hw4/saved_models/epoch19_52.7f1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gyODdCIdFayj"
      },
      "outputs": [],
      "source": [
        "  # Create the directory structure\n",
        "  !mkdir -p /content/drive/MyDrive/hw4/saved_models/epoch11_58.2f1\n",
        "  !mkdir -p /content/drive/MyDrive/hw4/saved_models/epoch19_52.7f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "r3OTxfxxF137"
      },
      "outputs": [],
      "source": [
        "  # Now copy the models\n",
        "  !cp -r /content/hw4/part-2-code/checkpoints/ft_experiments/a100_no_clipping/best_model/* \\\n",
        "    /content/drive/MyDrive/hw4/saved_models/epoch11_58.2f1/\n",
        "\n",
        "  !cp -r /content/hw4/part-2-code/checkpoints/ft_experiments/a100_no_clipping/latest_model/* \\\n",
        "    /content/drive/MyDrive/hw4/saved_models/epoch19_52.7f1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RFX4bVmF3oN",
        "outputId": "8a5f5895-e939-43d3-ebc4-bc2f74f881ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 231M\n",
            "-rw------- 1 root root 1.5K Nov 16 05:16 config.json\n",
            "-rw------- 1 root root  142 Nov 16 05:16 generation_config.json\n",
            "-rw------- 1 root root 231M Nov 16 05:16 model.safetensors\n"
          ]
        }
      ],
      "source": [
        "  # Verify it worked\n",
        "  !ls -lh /content/drive/MyDrive/hw4/saved_models/epoch11_58.2f1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xMFO1esF5ny",
        "outputId": "b8c566a9-ddfe-47a5-c8b0-47e345b85dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/hw4/part-2-code\n",
            "2025-11-17 22:17:26.680488: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-17 22:17:26.698751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763417846.720854    3661 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763417846.727508    3661 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763417846.744320    3661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763417846.744352    3661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763417846.744356    3661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763417846.744359    3661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-17 22:17:26.749398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "tokenizer_config.json: 2.32kB [00:00, 11.8MB/s]\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 37.8MB/s]\n",
            "tokenizer.json: 1.39MB [00:00, 140MB/s]\n",
            "Loading pretrained T5-small model...\n",
            "config.json: 1.21kB [00:00, 7.34MB/s]\n",
            "model.safetensors: 100% 242M/242M [00:01<00:00, 180MB/s]\n",
            "generation_config.json: 100% 147/147 [00:00<00:00, 1.61MB/s]\n",
            "Model loaded on cuda\n",
            "100% 265/265 [00:42<00:00,  6.29it/s]\n",
            "Epoch 0: Average train loss was 5.4085590488336415\n",
            "100% 30/30 [01:19<00:00,  2.64s/it]\n",
            "466it [00:00, 9462.41it/s]\n",
            "Epoch 0: Dev loss: 2.7843889511244244, Record F1: 0.11802575048283262, Record EM: 0.11802575107296137, SQL EM: 0.0\n",
            "Epoch 0: 99.57% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:41<00:00,  6.42it/s]\n",
            "Epoch 1: Average train loss was 2.4741217088338483\n",
            "100% 30/30 [03:02<00:00,  6.08s/it]\n",
            "465it [02:00,  3.87it/s] \n",
            "Epoch 1: Dev loss: 1.9842179029225866, Record F1: 0.13720369597574233, Record EM: 0.12017167381974249, SQL EM: 0.0\n",
            "Epoch 1: 40.34% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:40<00:00,  6.47it/s]\n",
            "Epoch 2: Average train loss was 1.9604847892367219\n",
            "100% 30/30 [03:01<00:00,  6.05s/it]\n",
            "463it [02:00,  3.86it/s]\n",
            "Epoch 2: Dev loss: 1.7646722675403923, Record F1: 0.1789934870607472, Record EM: 0.15450643776824036, SQL EM: 0.0\n",
            "Epoch 2: 10.94% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:41<00:00,  6.40it/s]\n",
            "Epoch 3: Average train loss was 1.7713179598650408\n",
            "100% 30/30 [02:58<00:00,  5.95s/it]\n",
            "466it [00:15, 29.61it/s]\n",
            "Epoch 3: Dev loss: 1.6571207031200996, Record F1: 0.20034313610943733, Record EM: 0.16738197424892703, SQL EM: 0.0\n",
            "Epoch 3: 14.38% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:41<00:00,  6.46it/s]\n",
            "Epoch 4: Average train loss was 1.6810994877178451\n",
            "100% 30/30 [02:20<00:00,  4.69s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 4: Dev loss: 1.6115784621865599, Record F1: 0.22636166095706017, Record EM: 0.20386266094420602, SQL EM: 0.0\n",
            "Epoch 4: 16.09% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:41<00:00,  6.39it/s]\n",
            "Epoch 5: Average train loss was 1.6299311002713728\n",
            "100% 30/30 [02:34<00:00,  5.16s/it]\n",
            "466it [00:43, 10.73it/s]\n",
            "Epoch 5: Dev loss: 1.5759360865346124, Record F1: 0.24962525538319638, Record EM: 0.2317596566523605, SQL EM: 0.0\n",
            "Epoch 5: 16.95% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:40<00:00,  6.48it/s]\n",
            "Epoch 6: Average train loss was 1.596431132893009\n",
            "100% 30/30 [02:25<00:00,  4.86s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 6: Dev loss: 1.5550046640163133, Record F1: 0.26712667644013366, Record EM: 0.24248927038626608, SQL EM: 0.0\n",
            "Epoch 6: 13.52% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:42<00:00,  6.29it/s]\n",
            "Epoch 7: Average train loss was 1.5727649758360411\n",
            "100% 30/30 [02:18<00:00,  4.60s/it]\n",
            "466it [01:59,  3.88it/s]\n",
            "Epoch 7: Dev loss: 1.5381492290930041, Record F1: 0.2827237509655723, Record EM: 0.2575107296137339, SQL EM: 0.0\n",
            "Epoch 7: 12.02% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:41<00:00,  6.36it/s]\n",
            "Epoch 8: Average train loss was 1.555476789090498\n",
            "100% 30/30 [02:36<00:00,  5.22s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 8: Dev loss: 1.524178629562138, Record F1: 0.29531591500927257, Record EM: 0.26394849785407726, SQL EM: 0.0\n",
            "Epoch 8: 14.16% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:42<00:00,  6.27it/s]\n",
            "Epoch 9: Average train loss was 1.5421042679116785\n",
            "100% 30/30 [02:50<00:00,  5.70s/it]\n",
            "466it [01:26,  5.42it/s]\n",
            "Epoch 9: Dev loss: 1.5126625190915424, Record F1: 0.3085972042863711, Record EM: 0.27467811158798283, SQL EM: 0.0\n",
            "Epoch 9: 8.58% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:42<00:00,  6.29it/s]\n",
            "Epoch 10: Average train loss was 1.530686123719803\n",
            "100% 30/30 [02:44<00:00,  5.48s/it]\n",
            "466it [01:21,  5.69it/s]\n",
            "Epoch 10: Dev loss: 1.505072625415263, Record F1: 0.314227880663935, Record EM: 0.27467811158798283, SQL EM: 0.0\n",
            "Epoch 10: 8.15% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:41<00:00,  6.37it/s]\n",
            "Epoch 11: Average train loss was 1.5219889014561414\n",
            "100% 30/30 [02:46<00:00,  5.56s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 11: Dev loss: 1.499575073368119, Record F1: 0.3232049321932945, Record EM: 0.2875536480686695, SQL EM: 0.0\n",
            "Epoch 11: 8.15% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:42<00:00,  6.27it/s]\n",
            "Epoch 12: Average train loss was 1.5147153304400562\n",
            "100% 30/30 [02:49<00:00,  5.66s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 12: Dev loss: 1.4933461096219884, Record F1: 0.32306756568607176, Record EM: 0.296137339055794, SQL EM: 0.0\n",
            "Epoch 12: 10.09% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "100% 265/265 [00:42<00:00,  6.22it/s]\n",
            "Epoch 13: Average train loss was 1.5086967038549128\n",
            "100% 30/30 [03:01<00:00,  6.05s/it]\n",
            "466it [01:32,  5.04it/s]\n",
            "Epoch 13: Dev loss: 1.489548703119762, Record F1: 0.3241831340099413, Record EM: 0.2982832618025751, SQL EM: 0.0\n",
            "Epoch 13: 11.59% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:42<00:00,  6.19it/s]\n",
            "Epoch 14: Average train loss was 1.503471579361594\n",
            "100% 30/30 [03:03<00:00,  6.11s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 14: Dev loss: 1.4853079126151694, Record F1: 0.3275807214326328, Record EM: 0.30686695278969955, SQL EM: 0.0\n",
            "Epoch 14: 16.52% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:47<00:00,  5.54it/s]\n",
            "Epoch 15: Average train loss was 1.4995116484208837\n",
            "100% 30/30 [03:20<00:00,  6.69s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 15: Dev loss: 1.48227801153254, Record F1: 0.3307627594853326, Record EM: 0.3111587982832618, SQL EM: 0.0\n",
            "Epoch 15: 12.23% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:49<00:00,  5.41it/s]\n",
            "Epoch 16: Average train loss was 1.4962091346644508\n",
            "100% 30/30 [03:58<00:00,  7.93s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 16: Dev loss: 1.4792598480979962, Record F1: 0.33532503147183, Record EM: 0.315450643776824, SQL EM: 0.0\n",
            "Epoch 16: 12.45% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:49<00:00,  5.41it/s]\n",
            "Epoch 17: Average train loss was 1.4937421249415195\n",
            "100% 30/30 [03:56<00:00,  7.89s/it]\n",
            "466it [01:00,  7.67it/s]\n",
            "Epoch 17: Dev loss: 1.4776213607834836, Record F1: 0.3385242855209523, Record EM: 0.3218884120171674, SQL EM: 0.0\n",
            "Epoch 17: 12.45% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:49<00:00,  5.37it/s]\n",
            "Epoch 18: Average train loss was 1.49146345033549\n",
            "100% 30/30 [03:48<00:00,  7.61s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 18: Dev loss: 1.4756560057248147, Record F1: 0.33926089053409886, Record EM: 0.31759656652360513, SQL EM: 0.0\n",
            "Epoch 18: 11.59% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:49<00:00,  5.35it/s]\n",
            "Epoch 19: Average train loss was 1.4896948798424419\n",
            "100% 30/30 [04:19<00:00,  8.64s/it]\n",
            "465it [02:00,  3.87it/s]\n",
            "Epoch 19: Dev loss: 1.4752579998942603, Record F1: 0.3304636911242189, Record EM: 0.3133047210300429, SQL EM: 0.0\n",
            "Epoch 19: 12.88% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "100% 265/265 [00:49<00:00,  5.32it/s]\n",
            "Epoch 20: Average train loss was 1.4889304578273928\n",
            "100% 30/30 [04:23<00:00,  8.79s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 20: Dev loss: 1.4740110854712023, Record F1: 0.3349319908516409, Record EM: 0.31759656652360513, SQL EM: 0.0\n",
            "Epoch 20: 13.09% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "100% 265/265 [00:51<00:00,  5.19it/s]\n",
            "Epoch 21: Average train loss was 1.4880978877346476\n",
            "100% 30/30 [04:51<00:00,  9.72s/it]\n",
            "466it [01:04,  7.26it/s]\n",
            "Epoch 21: Dev loss: 1.4737066886725665, Record F1: 0.3386873556377191, Record EM: 0.3197424892703863, SQL EM: 0.0\n",
            "Epoch 21: 12.23% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "100% 265/265 [00:50<00:00,  5.23it/s]\n",
            "Epoch 22: Average train loss was 1.4872769361867344\n",
            "100% 30/30 [04:49<00:00,  9.66s/it]\n",
            "466it [01:07,  6.95it/s]\n",
            "Epoch 22: Dev loss: 1.4731631375063858, Record F1: 0.33951271053874005, Record EM: 0.3218884120171674, SQL EM: 0.0\n",
            "Epoch 22: 12.02% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/best_model\n",
            "100% 265/265 [00:49<00:00,  5.39it/s]\n",
            "Epoch 23: Average train loss was 1.487354463307689\n",
            "100% 30/30 [04:55<00:00,  9.83s/it]\n",
            "466it [01:05,  7.14it/s]\n",
            "Epoch 23: Dev loss: 1.4733227526157993, Record F1: 0.33951271053874005, Record EM: 0.3218884120171674, SQL EM: 0.0\n",
            "Epoch 23: 12.66% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "100% 265/265 [00:50<00:00,  5.22it/s]\n",
            "Epoch 24: Average train loss was 1.4873580623232474\n",
            "100% 30/30 [04:49<00:00,  9.64s/it]\n",
            "466it [01:07,  6.93it/s]\n",
            "Epoch 24: Dev loss: 1.4733221422400704, Record F1: 0.33951271053874005, Record EM: 0.3218884120171674, SQL EM: 0.0\n",
            "Epoch 24: 12.88% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/final_optimized_65plus/latest_model\n",
            "Loading model from checkpoints/ft_experiments/final_optimized_65plus/best_model...\n",
            "100% 30/30 [04:46<00:00,  9.55s/it]\n",
            "466it [01:07,  6.89it/s]\n",
            "Dev set results: Loss: 1.4731631375063858, Record F1: 0.33951271053874005, Record EM: 0.3218884120171674, SQL EM: 0.0\n",
            "Dev set results: 12.02% of the generated outputs led to SQL errors\n",
            "100% 27/27 [03:20<00:00,  7.44s/it]\n",
            "432it [01:23,  5.18it/s]\n",
            "Test predictions saved to results/t5_ft_final_optimized_65plus_test.sql and records/t5_ft_final_optimized_65plus_test.pkl\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%cd /content/hw4/part-2-code\n",
        "!python train_t5.py \\\n",
        "  --finetune \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --weight_decay 0.05 \\\n",
        "  --scheduler_type cosine \\\n",
        "  --num_warmup_epochs 3 \\\n",
        "  --max_n_epochs 25 \\\n",
        "  --patience_epochs 8 \\\n",
        "  --batch_size 16 \\\n",
        "  --test_batch_size 16 \\\n",
        "  --experiment_name final_optimized_65plus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7wwvEbX4tG3",
        "outputId": "2fdc5b87-c741-4d3d-9e42-02ca5d7674b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-18 02:05:56.844544: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-18 02:05:56.862839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763431556.885100   60276 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763431556.891871   60276 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763431556.908772   60276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763431556.908806   60276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763431556.908809   60276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763431556.908811   60276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-18 02:05:56.913841: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Loading pretrained T5-small model...\n",
            "Model loaded on cuda\n",
            "100% 265/265 [00:41<00:00,  6.41it/s]\n",
            "Epoch 0: Average train loss was 1.5465146656444466\n",
            "100% 30/30 [03:05<00:00,  6.17s/it]\n",
            "466it [00:03, 123.65it/s]\n",
            "Epoch 0: Dev loss: 0.3419049748197363, Record F1: 0.18063582540245207, Record EM: 0.13304721030042918, SQL EM: 0.0\n",
            "Epoch 0: 61.59% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [00:41<00:00,  6.43it/s]\n",
            "Epoch 1: Average train loss was 0.3815540852225891\n",
            "100% 30/30 [02:41<00:00,  5.37s/it]\n",
            "466it [00:29, 15.95it/s]\n",
            "Epoch 1: Dev loss: 0.18797281511130448, Record F1: 0.29075285905405757, Record EM: 0.21888412017167383, SQL EM: 0.0\n",
            "Epoch 1: 30.04% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [00:40<00:00,  6.49it/s]\n",
            "Epoch 2: Average train loss was 0.24030123013543983\n",
            "100% 30/30 [01:52<00:00,  3.75s/it]\n",
            "464it [02:00,  3.87it/s]\n",
            "Epoch 2: Dev loss: 0.13182019885403326, Record F1: 0.3935513186552182, Record EM: 0.2875536480686695, SQL EM: 0.004291845493562232\n",
            "Epoch 2: 17.60% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [00:40<00:00,  6.47it/s]\n",
            "Epoch 3: Average train loss was 0.17776791956371654\n",
            "100% 30/30 [02:00<00:00,  4.01s/it]\n",
            "461it [02:00,  3.84it/s]\n",
            "Epoch 3: Dev loss: 0.10345819066376313, Record F1: 0.4746167090359997, Record EM: 0.37553648068669526, SQL EM: 0.008583690987124463\n",
            "Epoch 3: 20.60% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [00:41<00:00,  6.34it/s]\n",
            "Epoch 4: Average train loss was 0.1449986232604119\n",
            "100% 30/30 [02:16<00:00,  4.55s/it]\n",
            "460it [02:00,  3.83it/s]\n",
            "Epoch 4: Dev loss: 0.08781007789490891, Record F1: 0.5106071997432908, Record EM: 0.4055793991416309, SQL EM: 0.019313304721030045\n",
            "Epoch 4: 18.88% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [00:45<00:00,  5.83it/s]\n",
            "Epoch 5: Average train loss was 0.12233791290596385\n",
            "100% 30/30 [03:06<00:00,  6.21s/it]\n",
            "462it [02:00,  3.85it/s]\n",
            "Epoch 5: Dev loss: 0.0761932167074165, Record F1: 0.5619922596689102, Record EM: 0.4570815450643777, SQL EM: 0.019313304721030045\n",
            "Epoch 5: 15.02% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [00:47<00:00,  5.53it/s]\n",
            "Epoch 6: Average train loss was 0.10721056999556433\n",
            "100% 30/30 [04:08<00:00,  8.29s/it]\n",
            "270it [02:00,  2.25it/s]\n",
            "Epoch 6: Dev loss: 0.06806196678088604, Record F1: 0.4073857000511801, Record EM: 0.3540772532188841, SQL EM: 0.02145922746781116\n",
            "Epoch 6: 50.64% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:02<00:00,  4.24it/s]\n",
            "Epoch 7: Average train loss was 0.09453765098142007\n",
            "100% 30/30 [06:34<00:00, 13.16s/it]\n",
            "370it [02:00,  3.08it/s]\n",
            "Epoch 7: Dev loss: 0.06230132201065178, Record F1: 0.4993186586485172, Record EM: 0.41630901287553645, SQL EM: 0.019313304721030045\n",
            "Epoch 7: 32.83% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:10<00:00,  3.74it/s]\n",
            "Epoch 8: Average train loss was 0.08538890329181362\n",
            "100% 30/30 [08:50<00:00, 17.69s/it]\n",
            "334it [02:00,  2.78it/s]\n",
            "Epoch 8: Dev loss: 0.05588171613890044, Record F1: 0.4766432198445194, Record EM: 0.4206008583690987, SQL EM: 0.02145922746781116\n",
            "Epoch 8: 41.42% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:20<00:00,  3.29it/s]\n",
            "Epoch 9: Average train loss was 0.07729324209114984\n",
            "100% 30/30 [10:43<00:00, 21.44s/it]\n",
            "387it [02:00,  3.22it/s]\n",
            "Epoch 9: Dev loss: 0.051530369441298386, Record F1: 0.5478837193931086, Record EM: 0.48068669527896996, SQL EM: 0.02145922746781116\n",
            "Epoch 9: 32.19% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:34<00:00,  2.81it/s]\n",
            "Epoch 10: Average train loss was 0.07122873444465815\n",
            "100% 30/30 [12:17<00:00, 24.57s/it]\n",
            "387it [02:00,  3.22it/s]\n",
            "Epoch 10: Dev loss: 0.04808301135340534, Record F1: 0.5559002246767528, Record EM: 0.49141630901287553, SQL EM: 0.02145922746781116\n",
            "Epoch 10: 30.47% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:36<00:00,  2.74it/s]\n",
            "Epoch 11: Average train loss was 0.06675477106983045\n",
            "100% 30/30 [13:47<00:00, 27.59s/it]\n",
            "387it [02:00,  3.22it/s]\n",
            "Epoch 11: Dev loss: 0.04565752052856484, Record F1: 0.5826168232486231, Record EM: 0.51931330472103, SQL EM: 0.019313304721030045\n",
            "Epoch 11: 28.76% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [02:04<00:00,  2.13it/s]\n",
            "Epoch 12: Average train loss was 0.06286149766727855\n",
            "100% 30/30 [16:54<00:00, 33.82s/it]\n",
            "387it [02:00,  3.22it/s]\n",
            "Epoch 12: Dev loss: 0.043647893252093485, Record F1: 0.5748661319876754, Record EM: 0.5171673819742489, SQL EM: 0.019313304721030045\n",
            "Epoch 12: 29.61% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [02:10<00:00,  2.03it/s]\n",
            "Epoch 13: Average train loss was 0.05921267966676623\n",
            "100% 30/30 [17:31<00:00, 35.04s/it]\n",
            "411it [02:00,  3.42it/s]\n",
            "Epoch 13: Dev loss: 0.04155174325027973, Record F1: 0.6200076099250178, Record EM: 0.5600858369098712, SQL EM: 0.019313304721030045\n",
            "Epoch 13: 24.03% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [02:10<00:00,  2.03it/s]\n",
            "Epoch 14: Average train loss was 0.05582093842929369\n",
            "100% 30/30 [18:38<00:00, 37.30s/it]\n",
            "396it [02:00,  3.30it/s]\n",
            "Epoch 14: Dev loss: 0.039478806847346844, Record F1: 0.6111145447577764, Record EM: 0.5515021459227468, SQL EM: 0.019313304721030045\n",
            "Epoch 14: 26.39% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [02:13<00:00,  1.99it/s]\n",
            "Epoch 15: Average train loss was 0.05427181686893279\n",
            "100% 30/30 [19:31<00:00, 39.03s/it]\n",
            "402it [02:00,  3.35it/s]\n",
            "Epoch 15: Dev loss: 0.03895745471025679, Record F1: 0.609833310799375, Record EM: 0.5600858369098712, SQL EM: 0.019313304721030045\n",
            "Epoch 15: 26.18% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:52<00:00,  2.35it/s]\n",
            "Epoch 16: Average train loss was 0.05220047204652021\n",
            "100% 30/30 [16:59<00:00, 33.98s/it]\n",
            "396it [02:00,  3.30it/s]\n",
            "Epoch 16: Dev loss: 0.03758512165729705, Record F1: 0.6146726549720742, Record EM: 0.5622317596566524, SQL EM: 0.017167381974248927\n",
            "Epoch 16: 25.54% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [02:02<00:00,  2.16it/s]\n",
            "Epoch 17: Average train loss was 0.050496413675913975\n",
            "100% 30/30 [16:38<00:00, 33.29s/it]\n",
            "402it [02:00,  3.35it/s]\n",
            "Epoch 17: Dev loss: 0.03656679162508893, Record F1: 0.620103246781295, Record EM: 0.5643776824034334, SQL EM: 0.015021459227467811\n",
            "Epoch 17: 24.68% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [01:52<00:00,  2.35it/s]\n",
            "Epoch 18: Average train loss was 0.049153278639386205\n",
            "100% 30/30 [16:14<00:00, 32.47s/it]\n",
            "395it [02:00,  3.29it/s]\n",
            "Epoch 18: Dev loss: 0.03600222247202412, Record F1: 0.6131553339725078, Record EM: 0.5643776824034334, SQL EM: 0.015021459227467811\n",
            "Epoch 18: 26.18% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:59<00:00,  2.22it/s]\n",
            "Epoch 19: Average train loss was 0.047842743937265474\n",
            "100% 30/30 [16:34<00:00, 33.15s/it]\n",
            "420it [02:00,  3.50it/s]\n",
            "Epoch 19: Dev loss: 0.03562415866571298, Record F1: 0.643458663333419, Record EM: 0.5858369098712446, SQL EM: 0.015021459227467811\n",
            "Epoch 19: 21.46% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "100% 265/265 [01:49<00:00,  2.42it/s]\n",
            "Epoch 20: Average train loss was 0.047713802333554045\n",
            "100% 30/30 [17:06<00:00, 34.21s/it]\n",
            "396it [02:00,  3.30it/s]\n",
            "Epoch 20: Dev loss: 0.03533669444239724, Record F1: 0.6108523825782484, Record EM: 0.5579399141630901, SQL EM: 0.015021459227467811\n",
            "Epoch 20: 27.04% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:50<00:00,  2.39it/s]\n",
            "Epoch 21: Average train loss was 0.04723946728927745\n",
            "100% 30/30 [16:37<00:00, 33.23s/it]\n",
            "396it [02:00,  3.30it/s]\n",
            "Epoch 21: Dev loss: 0.035233815770853716, Record F1: 0.6164769692232267, Record EM: 0.5622317596566524, SQL EM: 0.015021459227467811\n",
            "Epoch 21: 25.32% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:55<00:00,  2.30it/s]\n",
            "Epoch 22: Average train loss was 0.04692652424546508\n",
            "100% 30/30 [16:41<00:00, 33.37s/it]\n",
            "396it [02:00,  3.30it/s]\n",
            "Epoch 22: Dev loss: 0.03513074324653943, Record F1: 0.6109728810675299, Record EM: 0.5622317596566524, SQL EM: 0.017167381974248927\n",
            "Epoch 22: 26.61% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [02:00<00:00,  2.20it/s]\n",
            "Epoch 23: Average train loss was 0.046677761283364594\n",
            "100% 30/30 [17:48<00:00, 35.60s/it]\n",
            "397it [02:00,  3.31it/s]\n",
            "Epoch 23: Dev loss: 0.03505674745343067, Record F1: 0.6211636598740028, Record EM: 0.5686695278969958, SQL EM: 0.017167381974248927\n",
            "Epoch 23: 25.54% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "100% 265/265 [01:58<00:00,  2.24it/s]\n",
            "Epoch 24: Average train loss was 0.046191917260293335\n",
            "100% 30/30 [16:15<00:00, 32.52s/it]\n",
            "411it [02:00,  3.42it/s]\n",
            "Epoch 24: Dev loss: 0.03505437117316694, Record F1: 0.6464520434323102, Record EM: 0.5901287553648069, SQL EM: 0.017167381974248927\n",
            "Epoch 24: 22.53% of the generated outputs led to SQL errors\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/latest_model\n",
            "Model saved to checkpoints/ft_experiments/t5_final_verified/best_model\n",
            "Loading model from checkpoints/ft_experiments/t5_final_verified/best_model...\n",
            "100% 30/30 [15:51<00:00, 31.71s/it]\n",
            "398it [02:00,  3.32it/s]\n",
            "Dev set results: Loss: 0.03505437117316694, Record F1: 0.624768449414282, Record EM: 0.572961373390558, SQL EM: 0.017167381974248927\n",
            "Dev set results: 25.32% of the generated outputs led to SQL errors\n",
            "100% 27/27 [11:11<00:00, 24.86s/it]\n",
            "367it [02:00,  3.06it/s]\n",
            "Test predictions saved to results/t5_ft_t5_final_verified_test.sql and records/t5_ft_t5_final_verified_test.pkl\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.12/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1594, in _shutdown\n",
            "    atexit_call()\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 31, in _python_exit\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1149, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_t5.py \\\n",
        "  --finetune \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --weight_decay 0.01 \\\n",
        "  --scheduler_type cosine \\\n",
        "  --max_n_epochs 25 \\\n",
        "  --patience_epochs 10 \\\n",
        "  --batch_size 16 \\\n",
        "  --test_batch_size 16 \\\n",
        "  --experiment_name t5_final_verified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n8L4yyNRDpx",
        "outputId": "66b61448-5c2d-41ea-cc62-d517e555469c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/train_t5.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python train_t5.py \\\n",
        "  --finetune \\\n",
        "  --learning_rate 1e-3 \\\n",
        "  --weight_decay 0.01 \\\n",
        "  --scheduler_type cosine \\\n",
        "  --num_warmup_epochs 2 \\\n",
        "  --max_n_epochs 30 \\\n",
        "  --patience_epochs 10 \\\n",
        "  --batch_size 16 \\\n",
        "  --test_batch_size 16 \\\n",
        "  --experiment_name t5_final_complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gs7mimgRDVw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USe0eI362NjP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "050df5f2dd4b43beada32764980efe07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15607f73ed5c42f3a9da18b59b438bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c8c915477c44a159be3154af507bcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2279912a52a8434bbf96d885c5ca8e34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d3e7c24339c4a56abb7e9330693398f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3665334ffc344048afec8f4c5a8a8d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_508d6f45c20f43ac9edcc4119eaec027",
            "placeholder": "​",
            "style": "IPY_MODEL_37124f01b2144e968ddae9dae4f89943",
            "value": " 2.32k/? [00:00&lt;00:00, 240kB/s]"
          }
        },
        "37124f01b2144e968ddae9dae4f89943": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "484fc1a74e9740a7aa7d74d5f29fb969": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4be5589f52244fc09c649e74aa7ab4e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4faebdf85df74ca6b36e0c0f73ed1196": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab9f1234743f45c8abae4acb98c3146f",
            "placeholder": "​",
            "style": "IPY_MODEL_ba3e850762294fb8bc440cd92823d125",
            "value": "tokenizer_config.json: "
          }
        },
        "508d6f45c20f43ac9edcc4119eaec027": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5259c41d265f4c18aed1ca589ddf56d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1f810ba555d495fbda97d447f7d2f12",
              "IPY_MODEL_f5ca95646d454852809e6b54e5ecc904",
              "IPY_MODEL_5fbbcbc6314e486dbf15d6790120c36e"
            ],
            "layout": "IPY_MODEL_050df5f2dd4b43beada32764980efe07"
          }
        },
        "5e2e870ec24948ebacdfbf1defe25a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6333b3aec7ba4a229efab525b20345e9",
              "IPY_MODEL_efb1aba2bc6f4b57ad2d87d35497d15e",
              "IPY_MODEL_9418fb2859bd4db7881ec0ea70d6601b"
            ],
            "layout": "IPY_MODEL_732305ee47ce423e8c2e65a7f828ab1f"
          }
        },
        "5fbbcbc6314e486dbf15d6790120c36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4be5589f52244fc09c649e74aa7ab4e9",
            "placeholder": "​",
            "style": "IPY_MODEL_484fc1a74e9740a7aa7d74d5f29fb969",
            "value": " 1.39M/? [00:00&lt;00:00, 58.9MB/s]"
          }
        },
        "60ac414cc5ee47d8aadded2868378a77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6333b3aec7ba4a229efab525b20345e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2279912a52a8434bbf96d885c5ca8e34",
            "placeholder": "​",
            "style": "IPY_MODEL_1c8c915477c44a159be3154af507bcd8",
            "value": "spiece.model: 100%"
          }
        },
        "732305ee47ce423e8c2e65a7f828ab1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84aea06d2222454391a3ca591f4605b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9418fb2859bd4db7881ec0ea70d6601b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae7ea71dcbb44254ac220f01deebd599",
            "placeholder": "​",
            "style": "IPY_MODEL_15607f73ed5c42f3a9da18b59b438bad",
            "value": " 792k/792k [00:00&lt;00:00, 22.4MB/s]"
          }
        },
        "a1f810ba555d495fbda97d447f7d2f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d3e7c24339c4a56abb7e9330693398f",
            "placeholder": "​",
            "style": "IPY_MODEL_a770bb39042b470aac9f41af48f50057",
            "value": "tokenizer.json: "
          }
        },
        "a770bb39042b470aac9f41af48f50057": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab9f1234743f45c8abae4acb98c3146f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae7ea71dcbb44254ac220f01deebd599": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1a84a81c8cc4eaea548c12817431975": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3c56b9250bd475babc5cc8db77e2abd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84aea06d2222454391a3ca591f4605b0",
            "value": 1
          }
        },
        "ba3e850762294fb8bc440cd92823d125": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3c56b9250bd475babc5cc8db77e2abd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d7969b8760a445deaa75c3feee13d296": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4faebdf85df74ca6b36e0c0f73ed1196",
              "IPY_MODEL_b1a84a81c8cc4eaea548c12817431975",
              "IPY_MODEL_3665334ffc344048afec8f4c5a8a8d6a"
            ],
            "layout": "IPY_MODEL_eb73559f9cc545469692d12be53c621e"
          }
        },
        "df236049975c4e14b2d5967f7d571703": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e51ca5eaa7a0413bb4e1b2f5f09df68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb73559f9cc545469692d12be53c621e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efb1aba2bc6f4b57ad2d87d35497d15e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60ac414cc5ee47d8aadded2868378a77",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e51ca5eaa7a0413bb4e1b2f5f09df68b",
            "value": 791656
          }
        },
        "f5ca95646d454852809e6b54e5ecc904": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df236049975c4e14b2d5967f7d571703",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fadf7fe651b04843b010b23a94559669",
            "value": 1
          }
        },
        "fadf7fe651b04843b010b23a94559669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
